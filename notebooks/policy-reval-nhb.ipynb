{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import matplotlib.animation as manimation\n",
    "\n",
    "import gym_env\n",
    "import utils\n",
    "from utils import create_mapping, get_transition_matrix, create_mapping_nb, get_full_maze_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "def gen_nhb_exp():\n",
    "    envstep=[]\n",
    "    for s in range(6):\n",
    "        # actions 0=left, 1=right\n",
    "        envstep.append([[0,0], [0,0]])  # [s', done]\n",
    "    envstep = np.array(envstep)\n",
    "    # State 0 -> 1, 2\n",
    "    envstep[0,0] = [1,0]\n",
    "    envstep[0,1] = [2,0]\n",
    "    # State 1 -> 3, 4\n",
    "    envstep[1,0] = [3, 1]\n",
    "    envstep[1,1] = [4, 1]\n",
    "    # State 2 -> 4, 5\n",
    "    envstep[2,0] = [4, 1]\n",
    "    envstep[2,1] = [5, 1]\n",
    "    \n",
    "    return envstep\n",
    "\n",
    "def policy_reval(agent):\n",
    "    \"\"\"\n",
    "    The New environment is the same as the old one except we \n",
    "    \n",
    "    Args:\n",
    "    agent (LinearRL class) : The LinearRL agent\n",
    "\n",
    "    Returns:\n",
    "    V_new (array) : New value of each state\n",
    "    \"\"\"\n",
    "    r_new = agent.r\n",
    "    expr_new = np.exp(r_new[agent.terminals] / agent._lambda)\n",
    "    Z_new = np.zeros(len(r_new))\n",
    "\n",
    "    Z_new[~agent.terminals] = agent.DR[~agent.terminals][:,~agent.terminals] @ agent.P @ expr_new\n",
    "    Z_new[agent.terminals] = expr_new\n",
    "    V_new = np.round(np.log(Z_new), 2)\n",
    "\n",
    "    return V_new, Z_new\n",
    "\n",
    "def decision_policy(agent, Z):\n",
    "    \"\"\"\n",
    "    Performs matrix version of equation 6 from the LinearRL paper\n",
    "\n",
    "    Args:\n",
    "    agent (LinearRL class) : The LinearRL agent\n",
    "\n",
    "    Returns:\n",
    "    pii (array) : The decision policy\n",
    "    \"\"\"\n",
    "    G = np.dot(agent.T, Z)\n",
    "\n",
    "    expv_tiled = np.tile(Z, (len(Z), 1))\n",
    "    G = G.reshape(-1, 1)\n",
    "    \n",
    "    zg = expv_tiled / G\n",
    "    pii = agent.T * zg\n",
    "\n",
    "    return pii\n",
    "\n",
    "def plot_decision_prob(probs_train, probs_test):\n",
    "    \"\"\"\n",
    "    Plots the decision probability of going towards a terminal state\n",
    "\n",
    "    Args:\n",
    "    probs_train (array) : Probability of heading towards each terminal state before policy revaluation\n",
    "    probs_test (array) : probability of heading towrads each terminal state after policy revaluation\n",
    "    \"\"\"\n",
    "    color_palette = sns.color_palette(\"colorblind\")\n",
    "    colors = [color_palette[3], color_palette[2]]\n",
    "    print(colors)\n",
    "    bar_positions_training = np.arange(len(probs_train)) * 0.4\n",
    "    bar_positions_test = np.arange(len(probs_train)) * 0.4 + 1\n",
    "    # bar_positions_training = np.array([0, 0.4])  # Bar positions for training (s1 and s2)\n",
    "    # bar_positions_test = np.array([1, 1.4])  # Bar positions for test (s1 and s2)\n",
    "\n",
    "    plt.bar(bar_positions_training, probs_train, width=0.3, color=colors, edgecolor='black')\n",
    "    plt.bar(bar_positions_test, probs_test, width=0.3, color=colors, edgecolor='black')\n",
    "\n",
    "    handles = [plt.Rectangle((0,0),1,1, facecolor=colors[i], edgecolor='black') for i in range(len(probs_train))]\n",
    "    plt.legend(handles, [f'State {i+1}' for i in range(len(probs_train))], title='States', loc='upper right')\n",
    "    \n",
    "    plt.ylabel('Probabilities')\n",
    "    plt.xticks([0.2, 1.2], ['Training', 'Test'])\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRL_NHB:\n",
    "    def __init__(self, alpha=0.25, beta=10, gamma=0.904, _lambda=10, epsilon=0.4, num_steps=25000, policy=\"softmax\", imp_samp=True, exp_type=\"policy_reval\"):\n",
    "        # Hard code start and end locations as well as size\n",
    "        self.start_loc = 0\n",
    "        self.target_locs = [3,4,5]\n",
    "        self.size = 6\n",
    "        self.agent_loc = self.start_loc\n",
    "        self.exp_type = exp_type\n",
    "\n",
    "        # Construct the transition probability matrix and envstep functions\n",
    "        self.T = self.construct_T()\n",
    "        self.envstep = gen_nhb_exp()\n",
    "        \n",
    "        # Get terminal states\n",
    "        self.terminals = np.diag(self.T) == 1\n",
    "        # Calculate P = T_{NT}\n",
    "        self.P = self.T[~self.terminals][:,self.terminals]\n",
    "\n",
    "        # Set reward\n",
    "        self.reward_nt = -1   # Non-terminal state reward (set to 0 for SR)\n",
    "        self.r = np.full(len(self.T), self.reward_nt)\n",
    "        # Reward of terminal states depends on if we are replicating reward revaluation or policy revaluation\n",
    "        if self.exp_type == \"policy_reval\":\n",
    "            self.r_term_1 = [0, 15, 30]\n",
    "            self.r_term_2 = [45, 15, 30]\n",
    "        elif self.exp_type == \"reward_reval\":\n",
    "            self.r_term_1 = [15, 0, 30]\n",
    "            self.r_term_2 = [45, 0, 30]\n",
    "        else:\n",
    "            print(\"Incorrect experiment type (exp_type)\")\n",
    "            return(0)\n",
    "        self.r[self.terminals] = self.r_term_1\n",
    "\n",
    "        # Precalculate exp(r) for use with LinearRL equations\n",
    "        self.expr_t = np.exp(self.r[self.terminals] / _lambda)\n",
    "        self.expr_nt = np.exp(self.reward_nt / _lambda)\n",
    "\n",
    "        # Params\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = self.expr_nt\n",
    "        self._lambda = _lambda\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.policy = policy\n",
    "        self.imp_samp = imp_samp\n",
    "\n",
    "        # Model\n",
    "        self.DR = self.get_DR()\n",
    "        self.Z = np.full(self.size, 0.01)\n",
    "\n",
    "        self.V = np.zeros(self.size)\n",
    "        self.one_hot = np.eye(self.size)\n",
    "\n",
    "    def construct_T(self):\n",
    "        \"\"\"\n",
    "        Manually construt the simple two-step task\n",
    "        \"\"\"\n",
    "        T = np.zeros((6, 6))\n",
    "        T[0,1] = 0.1\n",
    "        T[0,2] = 0.9\n",
    "        T[1,3] = 0.1\n",
    "        T[1,4] = 0.9\n",
    "        T[2,4] = 0.2\n",
    "        T[2,5] = 0.8\n",
    "        T[3:6, 3:6] = np.eye(3)\n",
    "\n",
    "        P = np.zeros((6, 6))\n",
    "        P[0, 1:3] = 0.5\n",
    "        P[1, 3:5] = 0.5\n",
    "        P[2, 4:6] = 0.5\n",
    "        P[3:6, 3:6] = np.eye(3)\n",
    "\n",
    "        return P\n",
    "    \n",
    "    def update_term(self):\n",
    "        \"\"\"\n",
    "        Update the terminal state values (experiment dependent)\n",
    "        \"\"\"\n",
    "        self.r[self.terminals] = self.r_term_2\n",
    "\n",
    "    def get_DR(self):\n",
    "        \"\"\"\n",
    "        Returns the DR initialization based on what decision policy we are using, values are filled with 0.01 if using softmax to avoid div by zero\n",
    "        \"\"\"\n",
    "        if self.policy == \"softmax\":\n",
    "            DR = np.full((self.size, self.size), 0.001)\n",
    "            np.fill_diagonal(DR, 1)\n",
    "            DR[np.where(self.terminals)[0], np.where(self.terminals)[0]] = (1/(1-self.gamma))\n",
    "        else:\n",
    "            DR = np.eye(self.size)\n",
    "\n",
    "        return DR\n",
    "\n",
    "    def update_Z(self):\n",
    "        self.Z[~self.terminals] = self.DR[~self.terminals][:,~self.terminals] @ self.P @ self.expr_t\n",
    "        self.Z[self.terminals] = self.expr_t\n",
    "\n",
    "    def update_V(self):\n",
    "        self.V = np.round(np.log(self.Z), 2)\n",
    "    \n",
    "    def get_successor_states(self, state):\n",
    "        \"\"\"\n",
    "        Manually define the successor states based on which state we are in\n",
    "        \"\"\"\n",
    "        return np.where(self.T[state, :] != 0)[0]\n",
    "\n",
    "    def importance_sampling(self, state, s_prob):\n",
    "        \"\"\"\n",
    "        Performs importance sampling P(x'|x)/u(x'|x). P(.) is the default policy, u(.) is the decision policy\n",
    "        \"\"\"\n",
    "        successor_states = self.get_successor_states(state)\n",
    "        p = 1/len(successor_states)\n",
    "        w = p/s_prob\n",
    "        # print(f\"state: {state}, s_prob: {s_prob}\")\n",
    "                \n",
    "        return w\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Action selection based on our policy\n",
    "        Options are: [random, softmax, egreedy, test]\n",
    "        \"\"\"\n",
    "        if self.policy == \"random\":\n",
    "            successor_states = self.get_successor_states(state)\n",
    "            return np.random.choice(successor_states)\n",
    "        \n",
    "        elif self.policy == \"softmax\":\n",
    "            successor_states = self.get_successor_states(state)\n",
    "            action_probs = np.full(2, 0.0)   # We can hardcode this because every state has 2 actions\n",
    "\n",
    "            v_sum = sum(np.exp((np.log(self.Z[s] + 1e-20)) / self.beta) for s in successor_states)\n",
    "\n",
    "            # if we don't have enough info, random action\n",
    "            if v_sum == 0:\n",
    "                return  np.random.choice([0,1])\n",
    "\n",
    "            for action in [0,1]:\n",
    "                new_state, _ = self.envstep[state, action]\n",
    "                action_probs[action] = np.exp((np.log(self.Z[new_state] + 1e-20)) / self.beta ) / v_sum\n",
    "\n",
    "            # print(f\"state: {state} | action_probs: {action_probs}\")\n",
    "            action = np.random.choice([0,1], p=action_probs)\n",
    "            s_prob = action_probs[action]\n",
    "\n",
    "            return action, s_prob\n",
    "\n",
    "    def get_D_inv(self):\n",
    "        \"\"\"\n",
    "        Calculates the DR directly using matrix inversion, used for testing\n",
    "        \"\"\"\n",
    "        I = np.eye(self.size)\n",
    "        D_inv = np.linalg.inv(I-self.gamma*self.T)\n",
    "\n",
    "        return D_inv\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Agent explores the maze according to its decision policy and and updates its DR as it goes\n",
    "        \"\"\"\n",
    "        # print(f\"Decision Policy: {self.policy}, Number of Iterations: {self.num_steps}, lr={self.alpha}, temperature={self.beta}, importance sampling={self.imp_samp}\")\n",
    "        # Iterate through number of steps\n",
    "        for i in range(self.num_steps):\n",
    "            # Agent gets some knowledge of terminal state values\n",
    "            if i == 2:\n",
    "                self.Z[self.terminals] = self.expr_t\n",
    "            # Current state\n",
    "            state = self.agent_loc\n",
    "\n",
    "            # Choose action\n",
    "            if self.policy == \"softmax\":\n",
    "                action, s_prob = self.select_action(state)\n",
    "            else:\n",
    "                action = self.select_action(state, self.policy)\n",
    "        \n",
    "            # Take action\n",
    "            next_state, done = self.envstep[state, action]\n",
    "            \n",
    "            # Importance sampling\n",
    "            if self.imp_samp:\n",
    "                w = self.importance_sampling(state, s_prob)\n",
    "                w = 1 if np.isnan(w) or w == 0 else w\n",
    "            else:\n",
    "                w = 1\n",
    "            \n",
    "            # Update default representation\n",
    "            target = self.one_hot[state] + self.gamma * self.DR[next_state]\n",
    "            self.DR[state] = (1 - self.alpha) * self.DR[state] + self.alpha * target * w\n",
    "\n",
    "            # Update Z-Values\n",
    "            self.Z[~self.terminals] = self.DR[~self.terminals][:,~self.terminals] @ self.P @ self.expr_t\n",
    "            \n",
    "            if done:\n",
    "                self.agent_loc = self.start_loc\n",
    "                continue\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            self.agent_loc = state\n",
    "\n",
    "        # Update DR at terminal state\n",
    "        self.update_Z()\n",
    "        self.update_V()\n",
    "        # self.Z[self.terminals] = np.exp(self.r[self.terminals] / self._lambda)\n",
    "        # self.V = np.round(np.log(self.Z), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Successor Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SR_NHB:\n",
    "    def __init__(self, alpha=0.1, beta=1, gamma=0.904, num_steps=25000, policy=\"random\", exp_type=\"policy_reval\"):\n",
    "        # Hard code start and end locations as well as size\n",
    "        self.start_loc = 0\n",
    "        self.target_locs = [3,4,5]\n",
    "        self.size = 6\n",
    "        self.agent_loc = self.start_loc\n",
    "        self.exp_type = exp_type\n",
    "\n",
    "        # Construct the transition probability matrix and envstep functions\n",
    "        self.T = self.construct_T()\n",
    "        self.envstep = gen_nhb_exp()\n",
    "        \n",
    "        # Get terminal states\n",
    "        self.terminals = np.diag(self.T) == 1\n",
    "\n",
    "        # Set reward\n",
    "        self.reward_nt = -1   # Non-terminal state reward (set to 0 for SR)\n",
    "        self.r = np.full(len(self.T), self.reward_nt)\n",
    "        # Reward of terminal states depends on if we are replicating reward revaluation or policy revaluation\n",
    "        if self.exp_type == \"policy_reval\":\n",
    "            self.r_term_1 = [0, 15, 30]\n",
    "            self.r_term_2 = [45, 15, 30]\n",
    "        elif self.exp_type == \"reward_reval\":\n",
    "            self.r_term_1 = [15, 0, 30]\n",
    "            self.r_term_2 = [45, 0, 30]\n",
    "        else:\n",
    "            print(\"Incorrect experiment type (exp_type)\")\n",
    "            return(0)\n",
    "        self.r[self.terminals] = self.r_term_1\n",
    "\n",
    "        # Params\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.num_steps = num_steps\n",
    "        self.policy = policy\n",
    "\n",
    "        # Model\n",
    "        self.SR = np.eye(self.size)\n",
    "        self.V = np.zeros(self.size)\n",
    "        self.one_hot = np.eye(self.size)\n",
    "\n",
    "    def construct_T(self):\n",
    "        \"\"\"\n",
    "        Manually construt the simple two-step task\n",
    "        \"\"\"\n",
    "        T = np.zeros((6, 6))\n",
    "        T[0,1] = 0.3\n",
    "        T[0,2] = 0.7\n",
    "        T[1,3] = 0.1\n",
    "        T[1,4] = 0.9\n",
    "        T[2,4] = 0.2\n",
    "        T[2,5] = 0.8\n",
    "        T[3:6, 3:6] = np.eye(3)\n",
    "\n",
    "        P = np.zeros((6, 6))\n",
    "        P[0, 1:3] = 0.5\n",
    "        P[1, 3:5] = 0.5\n",
    "        P[2, 4:6] = 0.5\n",
    "        P[3:6, 3:6] = np.eye(3)\n",
    "\n",
    "        return T\n",
    "    \n",
    "    def update_term(self):\n",
    "        \"\"\"\n",
    "        Update the terminal state values (experiment dependent)\n",
    "        \"\"\"\n",
    "        self.r[self.terminals] = self.r_term_2\n",
    "\n",
    "    def update_V(self):\n",
    "        self.V = self.SR @ self.r\n",
    "    \n",
    "    def get_successor_states(self, state):\n",
    "        \"\"\"\n",
    "        Manually define the successor states based on which state we are in\n",
    "        \"\"\"\n",
    "        return np.where(self.T[state, :] != 0)[0]\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Action selection based on our policy\n",
    "        Options are: [random, softmax, egreedy, test]\n",
    "        \"\"\"\n",
    "        if self.policy == \"random\":\n",
    "            successor_states = self.get_successor_states(state)\n",
    "            return np.random.choice(successor_states)\n",
    "        \n",
    "        elif self.policy == \"softmax\":\n",
    "            successor_states = self.get_successor_states(state)\n",
    "            action_probs = np.full(2, 0.0)   # We can hardcode this because every state has 2 actions\n",
    "\n",
    "            V = self.V[successor_states]\n",
    "            exp_V = np.exp(V / self.beta)\n",
    "            action_probs = exp_V / exp_V.sum()\n",
    "    \n",
    "            action = np.random.choice([0,1], p=action_probs)\n",
    "\n",
    "            return action\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Agent explores the maze according to its decision policy and and updates its DR as it goes\n",
    "        \"\"\"\n",
    "        # Iterate through number of steps\n",
    "        for i in range(self.num_steps):\n",
    "            # Current state\n",
    "            state = self.agent_loc\n",
    "\n",
    "            # Choose action\n",
    "            if self.policy == \"softmax\":\n",
    "                action = self.select_action(state)\n",
    "        \n",
    "            # Take action\n",
    "            next_state, done = self.envstep[state, action]\n",
    "            \n",
    "            # Update default representation\n",
    "            target = self.one_hot[state] + self.gamma * self.SR[next_state]\n",
    "            self.SR[state] = (1 - self.alpha) * self.SR[state] + self.alpha * target\n",
    "\n",
    "            # Update Values\n",
    "            self.update_V()\n",
    "\n",
    "            if done:\n",
    "                self.agent_loc = self.start_loc\n",
    "                continue\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            self.agent_loc = state\n",
    "\n",
    "        # Update DR at terminal state\n",
    "        self.update_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core LRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_lrl(T, c, M=None, D=None, lambd=1):\n",
    "    # reward vector across all states\n",
    "    r = -c\n",
    "\n",
    "    # terminal states\n",
    "    terminals = np.diag(T) == 1\n",
    "\n",
    "    # computing M (if not given)\n",
    "    if M is None:\n",
    "        ## M-Inv\n",
    "        L = np.diag(np.exp(c / lambd)) - T\n",
    "        L = L[~terminals][:, ~terminals]\n",
    "        M = np.linalg.inv(L)\n",
    "\n",
    "    # P = T_{NT}\n",
    "    P = T[~terminals][:, terminals]\n",
    "    expr = np.exp(r[terminals] / lambd)\n",
    "\n",
    "    expv_N = M @ P @ expr\n",
    "    expv = np.zeros(len(r))\n",
    "    expv[~terminals] = expv_N\n",
    "    expv[terminals] = np.exp(r[terminals] / lambd)\n",
    "\n",
    "    # A matrix formulation of equation 6 of manuscript\n",
    "    G = T @ expv\n",
    "    # Transpose expv and perform element-wise division\n",
    "    expv_tiled = np.tile(expv, (len(expv), 1))\n",
    "    G = G.reshape(-1, 1)\n",
    "    zg = expv_tiled / G\n",
    "    pii = T * zg\n",
    "\n",
    "    return pii, expv, M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.zeros((6, 6))\n",
    "T[0, 1:3] = 0.5\n",
    "T[1, 3:5] = 0.5\n",
    "T[2, 4:6] = 0.5\n",
    "T[3:6, 3:6] = np.eye(3)\n",
    "\n",
    "# T = np.zeros((6, 6))\n",
    "# T[0,1] = 0.1\n",
    "# T[0,2] = 0.9\n",
    "# T[1,3] = 0.1\n",
    "# T[1,4] = 0.9\n",
    "# T[2,4] = 0.1\n",
    "# T[2,5] = 0.9\n",
    "# T[3:6, 3:6] = np.eye(3)\n",
    "\n",
    "lambd = 100\n",
    "\n",
    "c1 = -np.array([0, 0, 0, 0, 15, 30]) / lambd\n",
    "U1, expv1, MNN = core_lrl(T, c1)\n",
    "\n",
    "c2 = np.copy(c1)\n",
    "c2[3] = -45 / lambd\n",
    "U2, expv2, _ = core_lrl(T,c2,MNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.5 0.5]\n",
      " [0.  1.  0. ]\n",
      " [0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "print(MNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.46257015, 0.53742985, 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.52083747, 0.47916253, 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DR-Inv Policy Revaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value before terminal change: \n",
      "[1.92 1.01 2.51 0.   1.5  3.  ]\n",
      "\n",
      "Value after terminal change: \n",
      "[3.29 3.86 2.51 4.5  1.5  3.  ]\n"
     ]
    }
   ],
   "source": [
    "# Load agent and solve for the DR using inverse equations\n",
    "agent = LinearRL_NHB()\n",
    "D_inv = agent.get_D_inv()\n",
    "# # Use inverse DR to get value function\n",
    "agent.DR = D_inv\n",
    "agent.update_Z()\n",
    "agent.update_V()\n",
    "print(\"Value before terminal change: \")\n",
    "print(agent.V)\n",
    "# # Update the terminal states and resolve for value function\n",
    "agent.update_term()\n",
    "V_new, _ = policy_reval(agent)\n",
    "print(\"\\nValue after terminal change: \")\n",
    "print(V_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DR-TD Policy Revaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_imp = LinearRL_NHB(_lambda=4, alpha=0.25, beta=4, num_steps=500, policy=\"softmax\", imp_samp=True)\n",
    "D_inv = agent_with_imp.get_D_inv()\n",
    "agent_with_imp.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Value before terminal change: \")\n",
    "# agent_with_imp.update_V()\n",
    "# print(agent_with_imp.V)\n",
    "# # Update the terminal states and resolve for value function\n",
    "# agent_with_imp.update_term()\n",
    "# agent_with_imp.learn_term(n=5)\n",
    "# # V_new = policy_reval(agent_with_imp)\n",
    "# print(\"\\nValue after terminal change: \")\n",
    "# # print(V_new)\n",
    "# print(agent_with_imp.V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_no_imp = LinearRL_NHB(_lambda=4, alpha=0.25, beta=4, num_steps=50, policy=\"softmax\", imp_samp=False)\n",
    "agent_no_imp.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Value before terminal change: \")\n",
    "# print(agent_no_imp.V)\n",
    "# # Update the terminal states and resolve for value function\n",
    "# agent_no_imp.update_term()\n",
    "# agent_no_imp.learn_term(n=5)\n",
    "# # V_new = policy_reval(agent_no_imp)\n",
    "# print(\"\\nValue after terminal change: \")\n",
    "# # print(V_new)\n",
    "# print(agent_no_imp.V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 500\n",
    "\n",
    "DR_avg_with_imp = np.zeros((6, 6))\n",
    "DR_avg_no_imp = np.zeros((6,6))\n",
    "SR_avg = np.zeros((6,6))\n",
    "# Construct a T for SR that prefers going to the right\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Define agents\n",
    "    agent_with_imp = LinearRL_NHB(_lambda=10, alpha=0.25, beta=1, num_steps=250, policy=\"softmax\", imp_samp=True, exp_type=\"policy_reval\")\n",
    "    agent_no_imp = LinearRL_NHB(_lambda=10, alpha=0.25, beta=1, num_steps=250, policy=\"softmax\", imp_samp=False, exp_type=\"policy_reval\")\n",
    "    agent_SR = SR_NHB(alpha=0.25, beta=10, num_steps=250, policy=\"softmax\")\n",
    "\n",
    "    # Have the agents learn the environment\n",
    "    agent_with_imp.learn()\n",
    "    agent_no_imp.learn()\n",
    "    agent_SR.learn()\n",
    "\n",
    "    # Add to the averages\n",
    "    DR_avg_with_imp += agent_with_imp.DR\n",
    "    DR_avg_no_imp += agent_no_imp.DR\n",
    "    SR_avg += agent_SR.SR\n",
    "\n",
    "# Take average\n",
    "DR_avg_with_imp /= num_iterations\n",
    "DR_avg_no_imp /= num_iterations\n",
    "SR_avg /= num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DR-Inv\n",
      "[[ 1.     0.452  0.452  2.151  4.302  2.151]\n",
      " [ 0.     1.     0.     4.754  4.754  0.   ]\n",
      " [ 0.     0.     1.     0.     4.754  4.754]\n",
      " [ 0.     0.     0.    10.508  0.     0.   ]\n",
      " [ 0.     0.     0.     0.    10.508  0.   ]\n",
      " [ 0.     0.     0.     0.     0.    10.508]]\n",
      "\n",
      " DR with Importance Sampling\n",
      "[[ 0.989  0.422  0.447  1.873  4.164  2.198]\n",
      " [ 0.001  0.986  0.001  4.561  4.78   0.001]\n",
      " [ 0.001  0.001  0.971  0.001  4.39   4.836]\n",
      " [ 0.001  0.001  0.001 10.508  0.001  0.001]\n",
      " [ 0.001  0.001  0.001  0.001 10.508  0.001]\n",
      " [ 0.001  0.001  0.001  0.001  0.001 10.508]]\n",
      "\n",
      " DR without Importance Sampling\n",
      "[[ 1.001  0.169  0.737  0.292  2.605  5.703]\n",
      " [ 0.001  1.001  0.001  1.662  7.824  0.001]\n",
      " [ 0.001  0.001  1.001  0.001  1.769  7.74 ]\n",
      " [ 0.001  0.001  0.001 10.508  0.001  0.001]\n",
      " [ 0.001  0.001  0.001  0.001 10.508  0.001]\n",
      " [ 0.001  0.001  0.001  0.001  0.001 10.508]]\n",
      "\n",
      " SR\n",
      "[[1.     0.1765 0.7275 0.0286 0.2481 0.5398]\n",
      " [0.     1.     0.     0.1947 0.7048 0.    ]\n",
      " [0.     0.     1.     0.     0.1594 0.7446]\n",
      " [0.     0.     0.     1.     0.     0.    ]\n",
      " [0.     0.     0.     0.     1.     0.    ]\n",
      " [0.     0.     0.     0.     0.     1.    ]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print(\"DR-Inv\")\n",
    "print(np.round(agent_with_imp.get_D_inv(), 3))\n",
    "print(\"\\n DR with Importance Sampling\")\n",
    "print(np.round(DR_avg_with_imp, 3))\n",
    "print(\"\\n DR without Importance Sampling\")\n",
    "print(np.round(DR_avg_no_imp, 3))\n",
    "print(\"\\n SR\")\n",
    "print(np.round(SR_avg, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, 45, 15, 30])"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_no_imp.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19.30070163 18.33294824 23.72929769 45.         15.         30.        ]\n",
      "[0.         0.24874657 0.75125343 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(SR_avg @ agent_no_imp.r)\n",
    "z = SR_avg @ agent_no_imp.r\n",
    "pii = decision_policy(agent_SR, z)\n",
    "print(pii[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 73.17917906 115.85442012  52.60259556 211.0929877   28.60947345\n",
      "  77.68684915]\n",
      "[0.         0.68773877 0.31226123 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "z = DR_avg_with_imp @ agent_with_imp.T @ np.exp(agent_no_imp.r/15)\n",
    "print(z)\n",
    "pii = decision_policy(agent_with_imp, z)\n",
    "print(pii[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 61.68462934  66.06602669  67.08709493 211.0929877   28.60947345\n",
      "  77.68684915]\n",
      "[0.         0.49616581 0.50383419 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "z = DR_avg_no_imp @ agent_no_imp.T @ np.exp(agent_no_imp.r/15)\n",
    "print(z)\n",
    "pii = decision_policy(agent_no_imp, z)\n",
    "print(pii[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 45 15 30]\n"
     ]
    }
   ],
   "source": [
    "agent_no_imp.update_term()\n",
    "print(agent_no_imp.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.73394269  2.66796797 12.09168045  1.          4.48168907 20.08553692]\n",
      "[25.91311927 44.31865082 12.48377843 90.0171313   4.48168907 20.08553692]\n"
     ]
    }
   ],
   "source": [
    "agent_with_imp.DR = DR_avg_with_imp\n",
    "agent_with_imp.update_Z()\n",
    "agent_with_imp.update_V()\n",
    "pii_old = decision_policy(agent_with_imp, agent_with_imp.Z)\n",
    "print(agent_with_imp.Z)\n",
    "agent_with_imp.update_term()\n",
    "V_new, Z_new = policy_reval(agent_with_imp)\n",
    "pii_new = decision_policy(agent_with_imp, Z_new)\n",
    "print(Z_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.17019523 0.82980477 0.         0.         0.        ]\n",
      "[0.         0.76692429 0.23307571 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(pii_old[0])\n",
    "print(pii_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_decision_prob(probs_train=[0.37420031, 0.62579969], probs_test=[0.87453662, 0.12546338])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.23768252  2.86368067 12.41955991  1.          4.48168907 20.08553692]\n",
      "[11.62140615 47.43227685 12.82229007 90.0171313   4.48168907 20.08553692]\n"
     ]
    }
   ],
   "source": [
    "agent_no_imp.DR = DR_avg_no_imp\n",
    "agent_with_imp.update_Z()\n",
    "agent_no_imp.update_V()\n",
    "pii_old = decision_policy(agent_no_imp, agent_no_imp.Z)\n",
    "print(agent_no_imp.Z)\n",
    "agent_no_imp.update_term()\n",
    "V_new, Z_new = policy_reval(agent_no_imp)\n",
    "pii_new = decision_policy(agent_no_imp, Z_new)\n",
    "print(Z_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.18737392 0.81262608 0.         0.         0.        ]\n",
      "[0.         0.78719804 0.21280196 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(pii_old[0])\n",
    "print(pii_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_decision_prob(probs_train=[0.23538717, 0.76461283], probs_test=[0.78205788, 0.21794212])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SR-TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_SR = SR_NHB(alpha=0.25, beta=10, num_steps=250, policy=\"softmax\")\n",
    "agent_SR.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.01222449, 12.32834614, 25.92754206,  0.        , 15.        ,\n",
       "       30.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_SR.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1,  0, 15, 30])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_SR.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.08067442 0.82332558 0.00183083 0.10285329 0.7124475 ]\n",
      " [0.         1.         0.         0.01476329 0.88855641 0.        ]\n",
      " [0.         0.         1.         0.         0.01283053 0.89116947]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(agent_SR.SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
