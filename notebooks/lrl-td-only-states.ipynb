{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import matplotlib.animation as manimation\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import gym_env\n",
    "import utils\n",
    "from utils import create_mapping, get_transition_matrix, create_mapping_nb, get_full_maze_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRL:\n",
    "    def __init__(self, env_name, alpha=0.1, beta=1, gamma=0.904, _lambda=1.0, epsilon=0.4, num_steps=25000, policy=\"random\"):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.start_loc = self.env.unwrapped.start_loc\n",
    "        self.target_loc = self.env.unwrapped.target_loc\n",
    "        self.maze = self.env.unwrapped.maze\n",
    "        self.walls = self.env.unwrapped.get_walls()\n",
    "        self.size = self.maze.size - len(self.walls)   # Size of the state space is the = size of maze - number of blocked states\n",
    "        self.height, self.width = self.maze.shape\n",
    "        self.target_locs = [self.target_loc]\n",
    "\n",
    "        # Create mapping and Transition matrix\n",
    "        self.mapping = create_mapping_nb(self.maze, self.walls)\n",
    "        self.reverse_mapping = {index: (i, j) for (i, j), index in self.mapping.items()}\n",
    "        self.T = get_transition_matrix(self.env, self.mapping)\n",
    "        \n",
    "\n",
    "        # Get terminal states\n",
    "        self.terminals = np.diag(self.T) == 1\n",
    "        # Calculate P = T_{NT}\n",
    "        self.P = self.T[~self.terminals][:,self.terminals]\n",
    "        # Set reward\n",
    "        self.reward_nt = -1\n",
    "        self.reward_t = -1\n",
    "        # self.r = np.full(len(self.T), -1)     # our reward at each non-terminal state to be -1\n",
    "        self.r = np.full(len(self.T), self.reward_nt)\n",
    "        # self.r[self.terminals] = -1           # reward at terminal state is -1\n",
    "        self.r[self.terminals] = self.reward_t\n",
    "        self.expr_t = np.exp(self.r[self.terminals] / _lambda)\n",
    "        # Precalculate exp(r) for use with LinearRL equations\n",
    "        self.expr_nt = np.exp(self.reward_nt / _lambda)\n",
    "\n",
    "        # Params\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = self.expr_nt\n",
    "        self._lambda = _lambda\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.policy = policy\n",
    "\n",
    "        # Model\n",
    "        self.DR = self.get_DR()\n",
    "        self.Z = np.full(self.size, 0.01)\n",
    "\n",
    "        self.V = np.zeros(self.size)\n",
    "        self.one_hot = np.eye(self.size)\n",
    "\n",
    "    def get_states(self):\n",
    "        \"\"\"\n",
    "        Returns all non-blocked states as well as a mapping of each state (i,j) -> to an index (k)\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        index_mapping = {}\n",
    "        index = 0\n",
    "        for i in range(len(self.maze)):\n",
    "            for j in range(len(self.maze[i])):\n",
    "                if self.maze[i][j] in ['0', 'S', 'G']:\n",
    "                    states.append((i, j))\n",
    "                    index_mapping[(i, j)] = index\n",
    "                    index += 1\n",
    "\n",
    "        return states, index_mapping\n",
    "\n",
    "    def get_DR(self):\n",
    "        \"\"\"\n",
    "        Returns the DR initialization based on what decision policy we are using, values are filled with 0.01 if using softmax to avoid div by zero\n",
    "        \"\"\"\n",
    "        if self.policy == \"random\":\n",
    "            DR = np.eye(self.size)\n",
    "            DR[np.where(self.terminals)[0], np.where(self.terminals)[0]] = (1/(1-self.gamma))\n",
    "        \n",
    "        elif self.policy == \"softmax\":\n",
    "            DR = np.full((self.size, self.size), 0.01)\n",
    "            np.fill_diagonal(DR, 1)\n",
    "            DR[np.where(self.terminals)[0], np.where(self.terminals)[0]] = (1/(1-self.gamma))\n",
    "\n",
    "        return DR\n",
    "\n",
    "    def update_V(self):\n",
    "        self.Z[~self.terminals] = self.DR[~self.terminals][:,~self.terminals] @ self.P @ self.expr_t\n",
    "        self.Z[self.terminals] = self.expr_t\n",
    "        self.V = np.round(np.log(self.Z), 2)\n",
    "    \n",
    "    def importance_sampling(self, state, s_prob):\n",
    "        \"\"\"\n",
    "        Performs importance sampling P(x'|x)/u(x'|x). P(.) is the default policy, u(.) us the decision policy\n",
    "        \"\"\"\n",
    "        successor_states = self.env.unwrapped.get_successor_states(state)\n",
    "        p = 1/len(successor_states)\n",
    "        w = p/s_prob\n",
    "                \n",
    "        return w\n",
    "\n",
    "    def select_action(self, state, beta=0.5, target_loc=None):\n",
    "        \"\"\"\n",
    "        Action selection based on our policy\n",
    "        Options are: [random, softmax, egreedy, test]\n",
    "        \"\"\"\n",
    "        if self.policy == \"random\":\n",
    "            return self.env.unwrapped.random_action()\n",
    "        \n",
    "        elif self.policy == \"softmax\":\n",
    "            successor_states = self.env.unwrapped.get_successor_states(state)      # succesor_states = [(state, terminated), ...]\n",
    "            action_probs = np.full(self.env.action_space.n, 0.0)\n",
    "\n",
    "            v_sum = sum(\n",
    "                        np.exp((np.log(self.Z[self.mapping[(s[0][0],s[0][1])]] + 1e-20)) / self.beta) for s in successor_states\n",
    "                        )\n",
    "\n",
    "            # if we don't have enough info, random action\n",
    "            if v_sum == 0:\n",
    "                return self.env.unwrapped.random_action() \n",
    "\n",
    "            for action in self.env.unwrapped.get_available_actions(state):\n",
    "                direction = self.env.unwrapped._action_to_direction[action]\n",
    "                new_state = state + direction\n",
    "                \n",
    "                action_probs[action] = np.exp((np.log(self.Z[self.mapping[(new_state[0], new_state[1])]] + 1e-20)) / self.beta ) / v_sum\n",
    "\n",
    "            action = np.random.choice(self.env.action_space.n, p=action_probs)\n",
    "            s_prob = action_probs[action]\n",
    "\n",
    "            return action, s_prob\n",
    "    \n",
    "        elif self.policy == \"egreedy\":\n",
    "            if np.random.uniform(low=0, high=1) < self.epsilon:\n",
    "                return self.env.unwrapped.random_action()\n",
    "            else:\n",
    "                action_values = np.full(self.env.action_space.n, -np.inf)\n",
    "                for action in self.env.unwrapped.get_available_actions(state):\n",
    "                    direction = self.env.unwrapped._action_to_direction[action]\n",
    "                    new_state = state + direction\n",
    "\n",
    "                    if self.maze[new_state[0], new_state[1]] == \"1\":\n",
    "                        continue\n",
    "\n",
    "                    action_values[action] = round(np.log(self.Z[self.mapping[(new_state[0],new_state[1])]]), 2)\n",
    "\n",
    "                return np.argmax(action_values)\n",
    "            \n",
    "        elif self.policy == \"test\":\n",
    "            action_values = np.full(self.env.action_space.n, -np.inf)\n",
    "            for action in self.env.unwrapped.get_available_actions(state):\n",
    "                direction = self.env.unwrapped._action_to_direction[action]\n",
    "                new_state = state + direction\n",
    "\n",
    "                # Need this to make it work for now\n",
    "                if np.array_equal(new_state, target_loc):\n",
    "                    return action\n",
    "\n",
    "                if self.maze[new_state[0], new_state[1]] == \"1\":\n",
    "                    continue\n",
    "                action_values[action] = round(np.log(self.Z[self.mapping[(new_state[0],new_state[1])]]), 2)\n",
    "\n",
    "            return np.nanargmax(action_values)\n",
    "\n",
    "    def get_D_inv(self):\n",
    "        \"\"\"\n",
    "        Calculates the DR directly using matrix inversion, used for testing\n",
    "        \"\"\"\n",
    "        I = np.eye(self.size)\n",
    "        D_inv = np.linalg.inv(I-self.gamma*self.T)\n",
    "\n",
    "        return D_inv\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Agent explores the maze according to its decision policy and and updates its DR as it goes\n",
    "        \"\"\"\n",
    "        print(f\"Decision Policy: {self.policy}, Number of Iterations: {self.num_steps}, lr={self.alpha}, temperature={self.beta}\")\n",
    "        self.env.reset()\n",
    "\n",
    "        D_inv_1 = self.get_D_inv()\n",
    "        D_inv_2 = np.linalg.inv(np.diag(np.exp(-self.r))-self.T)\n",
    "        diff_1, diff_2 = list(), list()\n",
    "\n",
    "        # Iterate through number of steps\n",
    "        for i in range(self.num_steps):\n",
    "            # Every 10 steps check difference\n",
    "            if (i + 1) % 10 == 0:\n",
    "                diff_1.append(np.abs(np.max(D_inv_1 - self.DR)))\n",
    "                diff_2.append(np.abs(np.max(D_inv_2 - (self.gamma * self.DR))))\n",
    "\n",
    "            # Current state\n",
    "            state = self.env.unwrapped.agent_loc\n",
    "            state_idx = self.mapping[(state[0], state[1])]\n",
    "\n",
    "            # Choose action\n",
    "            if self.policy == \"softmax\":\n",
    "                action, s_prob = self.select_action(state)\n",
    "            else:\n",
    "                action = self.select_action(state, self.policy)\n",
    "        \n",
    "            # Take action\n",
    "            obs, _, done, _, _ = self.env.step(action)\n",
    "\n",
    "            # Unpack observation to get new state\n",
    "            next_state = obs[\"agent\"]\n",
    "            next_state_idx = self.mapping[(next_state[0], next_state[1])]\n",
    "\n",
    "            # Importance sampling\n",
    "            if self.policy == \"softmax\":\n",
    "                w = self.importance_sampling(state, s_prob)\n",
    "                w = 1 if np.isnan(w) or w == 0 else w\n",
    "            else:\n",
    "                w = 1\n",
    "            \n",
    "            ## Update default representation\n",
    "            TDE = self.one_hot[state_idx] + self.gamma * self.DR[next_state_idx]\n",
    "            self.DR[state_idx] = (1 - self.alpha) * self.DR[state_idx] + self.alpha * TDE * w\n",
    "\n",
    "            ## Update Z-Values\n",
    "            self.Z = self.DR[:,~self.terminals] @ self.P @ self.expr_t\n",
    "\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "                continue\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "        # Update DR at terminal state\n",
    "        self.Z[self.terminals] = np.exp(self.r[self.terminals] / self._lambda)\n",
    "        self.V = np.round(np.log(self.Z), 2)\n",
    "\n",
    "        # return [diff_1, diff_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Different Temperature Values and Num Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature_values = [0.1, 0.4, 0.8, 1.0, 1.2]\n",
    "# num_steps = 50000\n",
    "\n",
    "# # Softmax agents\n",
    "# for temp in temperature_values:\n",
    "#     fig, ax = plt.subplots()\n",
    "#     # Define the agent\n",
    "#     agent = LinearRL(env_name=\"tolman-9x9-nb\", _lambda=1.0, alpha=0.001, beta=temp, num_steps=num_steps, policy=\"softmax\")\n",
    "#     # Get the diffs and set the x-axis\n",
    "#     diffs = agent.learn()\n",
    "#     x = np.arange(10, num_steps+1, 10)\n",
    "#     # Plot the diffs\n",
    "#     ax.plot(x, diffs[0], label='Diff 1')\n",
    "#     ax.plot(x, diffs[1], label='Diff 2')\n",
    "    \n",
    "#     # Add labels to the x-axis and y-axis\n",
    "#     ax.set_xlabel('Step Number')\n",
    "#     ax.set_ylabel('Difference')\n",
    "    \n",
    "#     # Add a title to the plot\n",
    "#     ax.set_title(f'Temperature: {temp}')\n",
    "    \n",
    "#     # Add a legend to the plot\n",
    "#     ax.legend()\n",
    "    \n",
    "#     # Show the plot\n",
    "#     plt.show()\n",
    "\n",
    "# # Random Agent\n",
    "# fig, ax = plt.subplots()\n",
    "# agent = LinearRL(env_name=\"tolman-9x9-nb\", _lambda=1.0, alpha=0.001, num_steps=num_steps, policy=\"random\")\n",
    "# diffs = agent.learn()\n",
    "# x = np.arange(10, num_steps+1, 10)\n",
    "\n",
    "# # Plot the diffs\n",
    "# ax.plot(x, diffs[0], label='Diff 1')\n",
    "# ax.plot(x, diffs[1], label='Diff 2')\n",
    "\n",
    "# # Add labels to the x-axis and y-axis\n",
    "# ax.set_xlabel('Step Number')\n",
    "# ax.set_ylabel('Difference')\n",
    "\n",
    "# # Add a title to the plot\n",
    "# ax.set_title(f'Temperature: {temp}')\n",
    "\n",
    "# # Add a legend to the plot\n",
    "# ax.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = LinearRL(env_name=\"tolman-9x9-nb\", _lambda=1.0, alpha=0.001, beta=1.0, num_steps=500000, policy=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1', '1', 'S', '1', '1', '1', '1', '1'],\n",
       "       ['1', '1', '1', '0', '1', '1', '1', '1', '1'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       "       ['0', '1', '1', '0', '1', '1', '1', '1', '0'],\n",
       "       ['0', '1', '1', '0', '1', '1', '1', '1', '0'],\n",
       "       ['0', '1', '1', '0', '1', '1', '1', '1', '0'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       "       ['1', '1', '1', '0', '1', '1', '1', '1', '1'],\n",
       "       ['1', '1', '1', 'G', '1', '1', '1', '1', '1']], dtype='<U21')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing DR-Inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 31)\n",
      "(31, 31)\n",
      "(31, 31)\n"
     ]
    }
   ],
   "source": [
    "D_inv_1 = agent.get_D_inv()\n",
    "print(D_inv_1.shape)\n",
    "D_inv_2 = np.linalg.inv(np.diag(np.exp(-agent.r / agent._lambda)) - agent.T)\n",
    "print(D_inv_2.shape)\n",
    "D_inv_3 = D_inv_1 * agent.gamma\n",
    "print(D_inv_3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maze Values using DR_1\n",
      "[[  -inf   -inf   -inf -13.95   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf -12.95   -inf   -inf   -inf   -inf   -inf]\n",
      " [-15.2  -14.57 -12.98 -11.33 -12.99 -14.64 -16.3  -17.89 -18.52]\n",
      " [-13.93   -inf   -inf  -9.     -inf   -inf   -inf   -inf -17.25]\n",
      " [-12.29   -inf   -inf  -7.32   -inf   -inf   -inf   -inf -15.61]\n",
      " [-10.64   -inf   -inf  -5.66   -inf   -inf   -inf   -inf -13.95]\n",
      " [ -8.98  -7.32  -5.66  -4.01  -5.66  -7.32  -8.98 -10.64 -12.29]\n",
      " [  -inf   -inf   -inf  -1.68   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf  -1.     -inf   -inf   -inf   -inf   -inf]]\n",
      "\n",
      " #################################################### \n",
      "\n",
      "Maze Values using DR_2\n",
      "[[  -inf   -inf   -inf -14.95   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf -13.95   -inf   -inf   -inf   -inf   -inf]\n",
      " [-16.2  -15.57 -13.98 -12.33 -13.99 -15.64 -17.3  -18.89 -19.52]\n",
      " [-14.93   -inf   -inf -10.     -inf   -inf   -inf   -inf -18.25]\n",
      " [-13.29   -inf   -inf  -8.32   -inf   -inf   -inf   -inf -16.61]\n",
      " [-11.64   -inf   -inf  -6.66   -inf   -inf   -inf   -inf -14.95]\n",
      " [ -9.98  -8.32  -6.66  -5.01  -6.66  -8.32  -9.98 -11.64 -13.29]\n",
      " [  -inf   -inf   -inf  -2.68   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf  -1.     -inf   -inf   -inf   -inf   -inf]]\n"
     ]
    }
   ],
   "source": [
    "agent.DR = D_inv_1\n",
    "agent.update_V()\n",
    "maze_values = get_full_maze_values(agent)\n",
    "print(\"Maze Values using DR_1\")\n",
    "print(maze_values)\n",
    "\n",
    "print(\"\\n #################################################### \\n\")\n",
    "\n",
    "agent.DR = D_inv_2\n",
    "agent.update_V()\n",
    "print(\"Maze Values using DR_2\")\n",
    "maze_values = get_full_maze_values(agent)\n",
    "print(maze_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff between D_1 and D_2: 1.0\n",
      "Max diff between D_3 and D_2: 1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max diff between D_1 and D_2: {np.max(np.abs(D_inv_1 - D_inv_2))}\")\n",
    "print(f\"Max diff between D_3 and D_2: {np.max(np.abs(D_inv_3 - D_inv_2))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Policy: random, Number of Iterations: 500000, lr=0.001, temperature=1.0\n"
     ]
    }
   ],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -inf   -inf   -inf -13.94   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf -12.94   -inf   -inf   -inf   -inf   -inf]\n",
      " [-15.27 -14.58 -12.96 -11.27 -12.96 -14.61 -16.28 -17.91 -18.69]\n",
      " [-14.     -inf   -inf  -8.96   -inf   -inf   -inf   -inf -17.46]\n",
      " [-12.32   -inf   -inf  -7.31   -inf   -inf   -inf   -inf -15.72]\n",
      " [-10.63   -inf   -inf  -5.68   -inf   -inf   -inf   -inf -14.04]\n",
      " [ -8.99  -7.3   -5.68  -4.03  -5.68  -7.34  -8.98 -10.67 -12.36]\n",
      " [  -inf   -inf   -inf  -1.68   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf  -1.     -inf   -inf   -inf   -inf   -inf]]\n"
     ]
    }
   ],
   "source": [
    "maze_values = get_full_maze_values(agent)\n",
    "print(maze_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_inv = agent.get_D_inv()\n",
    "# agent.DR = D_inv\n",
    "# agent.update_V()\n",
    "# maze_values = get_full_maze_values(agent)\n",
    "# print(maze_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00857426658209734\n"
     ]
    }
   ],
   "source": [
    "print(np.abs(np.max(D_inv - agent.DR)))\n",
    "\n",
    "# Calculate absolute difference\n",
    "diff = 0.02\n",
    "abs_diff = np.abs(D_inv - agent.DR)\n",
    "\n",
    "# Find indices where absolute difference is greater than 1\n",
    "indices = np.where(abs_diff > diff)\n",
    "\n",
    "# Print indices\n",
    "# print(f\"Indices with absolute difference greater than {diff}:\")\n",
    "# for i, j in zip(indices[0], indices[1]):\n",
    "#     print(f\"({i}, {j}) : {agent.reverse_mapping[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_agent = LinearRL(env_name=\"tolman-9x9-nb\", _lambda=1.0, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard Code new transition matrix to \"add a wall\"\n",
    "new_T = np.copy(agent.T)\n",
    "\n",
    "wall_between = [15, 12]  # Indices of the states that the wall is being put between\n",
    "wall_update = [[18], [5]]   # Indices of the states whose transition changes\n",
    "# Update transitions between wall states\n",
    "new_T[wall_between[0], wall_between[1]] = 0\n",
    "new_T[wall_between[1], wall_between[0]] = 0\n",
    "\n",
    "# Change transition probability for states affected\n",
    "for i in range(len(wall_between)):\n",
    "    prob_update = (1 - sum(new_T[wall_between[i]])) / np.count_nonzero(new_T[wall_between[i]])\n",
    "\n",
    "    for s in wall_update[i]:\n",
    "        new_T[wall_between[i], s] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve not blocked analytically\n",
    "T0 = agent.T\n",
    "T1 = new_T\n",
    "terminals = np.diag(T0) == 1\n",
    "I = np.eye(len(T0))\n",
    "\n",
    "# D0_1 = np.linalg.inv(I-agent.gamma*T0)\n",
    "# D0_1 = np.copy(agent.DR)\n",
    "# D0_1 *= agent.gamma\n",
    "D0_1 = agent.DR * agent.gamma\n",
    "D0_2 = np.linalg.inv(np.diag(np.exp(-agent.r))-T0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.163953413738653"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.abs(D0_1-D0_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff = 0.01\n",
    "# abs_diff = np.abs(D0_1-D0_2)\n",
    "# indices = np.where(abs_diff > diff)\n",
    "# print(f\"Indices with absolute difference greater than {diff}:\")\n",
    "# for i, j in zip(indices[0], indices[1]):\n",
    "#     print(f\"({i}, {j})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the DR to get Z-values\n",
    "# agent.DR = D0_1\n",
    "# agent.update_V()\n",
    "# new_maze_values = get_full_maze_values(agent)\n",
    "# print(new_maze_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.DR = D0_2\n",
    "# agent.update_V()\n",
    "# new_maze_values = get_full_maze_values(agent)\n",
    "# print(new_maze_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D1_1 = np.linalg.inv(I-agent.gamma*T1)\n",
    "D1_2 = np.linalg.inv(np.diag(np.exp(-agent.r))-T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_agent.DR = D1_1\n",
    "# new_agent.update_V()\n",
    "# new_maze_values = get_full_maze_values(new_agent)\n",
    "# print(new_maze_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -inf   -inf   -inf -24.88   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf -23.88   -inf   -inf   -inf   -inf   -inf]\n",
      " [-16.61 -18.27 -19.94 -22.25 -23.85 -24.48 -23.22 -21.58 -19.92]\n",
      " [-14.95   -inf   -inf -23.25   -inf   -inf   -inf   -inf -18.27]\n",
      " [-13.29   -inf   -inf  -7.63   -inf   -inf   -inf   -inf -16.61]\n",
      " [-11.64   -inf   -inf  -6.63   -inf   -inf   -inf   -inf -14.95]\n",
      " [ -9.98  -8.32  -6.66  -5.01  -6.66  -8.32  -9.98 -11.64 -13.29]\n",
      " [  -inf   -inf   -inf  -2.68   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf  -1.     -inf   -inf   -inf   -inf   -inf]]\n"
     ]
    }
   ],
   "source": [
    "new_agent.DR = D1_2\n",
    "new_agent.update_V()\n",
    "new_maze_values = get_full_maze_values(new_agent)\n",
    "print(new_maze_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_locs = [12, 15]\n",
    "L0 = np.diag(np.exp(-agent.r)) - T0\n",
    "L = np.diag(np.exp(-agent.r)) - T1\n",
    "\n",
    "delta = L[delta_locs, :] - L0[delta_locs, :]\n",
    "D0_j = D0_1[:,delta_locs]\n",
    "I = np.eye(len(delta_locs))\n",
    "\n",
    "inv = np.linalg.inv(I + np.dot(delta, D0_j))\n",
    "\n",
    "B = np.dot( np.dot(D0_j, inv), np.dot(delta, D0_1) )\n",
    "D = D0_1 - B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -inf   -inf   -inf -19.02   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf -17.84   -inf   -inf   -inf   -inf   -inf]\n",
      " [-16.73 -18.14 -17.76 -15.85 -17.84 -19.58 -21.73 -22.19 -20.18]\n",
      " [-15.02   -inf   -inf -13.28   -inf   -inf   -inf   -inf -18.48]\n",
      " [-13.32   -inf   -inf  -7.63   -inf   -inf   -inf   -inf -16.72]\n",
      " [-11.63   -inf   -inf  -6.64   -inf   -inf   -inf   -inf -15.04]\n",
      " [ -9.99  -8.3   -6.68  -5.03  -6.68  -8.34  -9.98 -11.67 -13.36]\n",
      " [  -inf   -inf   -inf  -2.68   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf  -1.     -inf   -inf   -inf   -inf   -inf]]\n"
     ]
    }
   ],
   "source": [
    "new_agent.DR = D\n",
    "new_agent.update_V()\n",
    "new_maze_values = get_full_maze_values(new_agent)\n",
    "print(new_maze_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find states whose transition has been changed\n",
    "differences = agent.T != new_T\n",
    "different_rows, different_cols = np.where(differences)\n",
    "delta_locs = np.unique(different_rows)\n",
    "delta_locs = [12, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 15]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "D0 = D_inv\n",
    "L0 = np.diag(np.exp(-agent.r)) - agent.T\n",
    "L = np.diag(np.exp(-agent.r)) - new_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use locations of change to get different values\n",
    "delta = L[delta_locs, :] - L0[delta_locs, :]\n",
    "D0_j = D0[:,delta_locs]\n",
    "I = np.eye(len(delta_locs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inverse in eqn (17)\n",
    "inv = np.linalg.inv(I + np.dot(delta, D0_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use everything to get B and update the DR\n",
    "B = np.dot( np.dot(D0_j, inv), np.dot(delta, D0) )\n",
    "D = D0 - B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_agent.DR = D\n",
    "new_agent.P = new_T[~new_agent.terminals][:,new_agent.terminals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/4_xlxj4j2r1_sx5v07843wbw0000gn/T/ipykernel_85511/1242637644.py:74: RuntimeWarning: invalid value encountered in log\n",
      "  self.V = np.round(np.log(self.Z), 2)\n"
     ]
    }
   ],
   "source": [
    "new_agent.update_V()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_maze_values = get_full_maze_values(new_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -inf   -inf   -inf    nan   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf    nan   -inf   -inf   -inf   -inf   -inf]\n",
      " [   nan    nan    nan -15.25    nan    nan    nan    nan    nan]\n",
      " [ -8.11   -inf   -inf -16.25   -inf   -inf   -inf   -inf -11.43]\n",
      " [ -6.3    -inf   -inf  -0.63   -inf   -inf   -inf   -inf  -9.61]\n",
      " [ -4.64   -inf   -inf   0.37   -inf   -inf   -inf   -inf  -7.95]\n",
      " [ -2.98  -1.32   0.34   1.99   0.34  -1.32  -2.98  -4.63  -6.29]\n",
      " [  -inf   -inf   -inf   4.32   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf   5.     -inf   -inf   -inf   -inf   -inf]]\n"
     ]
    }
   ],
   "source": [
    "print(new_maze_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(agent.size)\n",
    "D_inv_new = np.linalg.inv(I-agent.gamma*new_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -inf   -inf   -inf -17.88   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf -16.88   -inf   -inf   -inf   -inf   -inf]\n",
      " [ -9.61 -11.27 -12.94 -15.25 -16.85 -17.48 -16.22 -14.58 -12.92]\n",
      " [ -7.95   -inf   -inf -16.25   -inf   -inf   -inf   -inf -11.27]\n",
      " [ -6.29   -inf   -inf  -0.63   -inf   -inf   -inf   -inf  -9.61]\n",
      " [ -4.64   -inf   -inf   0.37   -inf   -inf   -inf   -inf  -7.95]\n",
      " [ -2.98  -1.32   0.34   1.99   0.34  -1.32  -2.98  -4.64  -6.29]\n",
      " [  -inf   -inf   -inf   4.32   -inf   -inf   -inf   -inf   -inf]\n",
      " [  -inf   -inf   -inf   5.     -inf   -inf   -inf   -inf   -inf]]\n"
     ]
    }
   ],
   "source": [
    "new_agent.DR = D_inv_new\n",
    "new_agent.update_V()\n",
    "new_maze_values = get_full_maze_values(new_agent)\n",
    "print(new_maze_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5081131425142636\n",
      "Indices with absolute difference greater than 1:\n",
      "(5, 5)\n",
      "(5, 12)\n",
      "(5, 15)\n",
      "(5, 18)\n",
      "(12, 1)\n",
      "(12, 4)\n",
      "(12, 5)\n",
      "(12, 6)\n",
      "(12, 12)\n",
      "(12, 15)\n",
      "(12, 18)\n",
      "(12, 23)\n",
      "(15, 1)\n",
      "(15, 4)\n",
      "(15, 5)\n",
      "(15, 6)\n",
      "(15, 12)\n",
      "(15, 15)\n",
      "(15, 18)\n",
      "(15, 23)\n",
      "(18, 5)\n",
      "(18, 12)\n",
      "(18, 15)\n",
      "(18, 18)\n",
      "(18, 23)\n"
     ]
    }
   ],
   "source": [
    "print(np.abs(np.max(D - D_inv_new)))\n",
    "# Calculate absolute difference\n",
    "abs_diff = np.abs(D - D_inv_new)\n",
    "\n",
    "# Find indices where absolute difference is greater than 1\n",
    "indices = np.where(abs_diff > 0.01)\n",
    "\n",
    "# Print indices\n",
    "print(\"Indices with absolute difference greater than 1:\")\n",
    "for i, j in zip(indices[0], indices[1]):\n",
    "    print(f\"({i}, {j})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "D[12] = D_inv_new[12]\n",
    "D[:,12] = D_inv_new[:,12]\n",
    "D[15] = D_inv_new[15]\n",
    "D[:,15] = D_inv_new[:,15]\n",
    "D[5] = D_inv_new[5]\n",
    "D[:,5] = D_inv_new[:,5]\n",
    "D[18] = D_inv_new[18]\n",
    "D[:,18] = D_inv_new[:,18]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5081131425142636\n"
     ]
    }
   ],
   "source": [
    "print(np.abs(np.max(D[15] - D_inv_new[15])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.72090306e-08, 4.67789950e-08, 6.71258563e-05, 1.27877934e-05,\n",
       "       2.39579672e-06, 2.37107954e-07, 4.83021113e-08, 2.54895490e-08,\n",
       "       9.02734445e-08, 4.65287778e-07, 2.43929318e-06, 3.52146197e-04,\n",
       "       8.72271415e-08, 1.27960849e-05, 1.84733936e-03, 5.32688839e-01,\n",
       "       6.71274368e-05, 9.69103184e-03, 1.44799839e+00, 3.52146499e-04,\n",
       "       5.08385721e-02, 2.66696102e-01, 1.39907176e+00, 7.33944659e+00,\n",
       "       1.39907176e+00, 2.66696102e-01, 5.08385721e-02, 9.69103185e-03,\n",
       "       1.84733942e-03, 7.55565953e+01, 1.48413159e+02])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_agent.Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.87739399e-03, -7.82156779e-03, -2.07375712e-04, -1.42779619e-03,\n",
       "       -7.55492918e-03, -3.96450572e-02, -7.55728058e-03, -1.44057975e-03,\n",
       "       -2.74522948e-04, -5.18817279e-05, -7.53536850e-06,  3.00384936e-04,\n",
       "       -4.08131977e-01,  1.09152174e-05,  1.84043754e-03,  1.20552050e+00,\n",
       "        6.68766427e-05,  9.70527088e-03,  1.57400742e+00,  3.52663908e-04,\n",
       "        5.09228854e-02,  2.67140237e-01,  1.40140202e+00,  7.35167105e+00,\n",
       "        1.40140203e+00,  2.67140306e-01,  5.09232475e-02,  9.70717056e-03,\n",
       "        1.85040314e-03,  7.55588439e+01,  1.48413159e+02])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_agent.Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
