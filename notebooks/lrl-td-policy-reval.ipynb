{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import matplotlib.animation as manimation\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import gym_env\n",
    "import utils\n",
    "from utils import create_mapping, get_transition_matrix, create_mapping_nb, get_full_maze_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_reval(agent, r_new):\n",
    "    \"\"\"\n",
    "    The New environment is the same as the old one except we \n",
    "    \n",
    "    Args:\n",
    "    agent (LinearRL class) : The LinearRL agent\n",
    "    r_new (array) : Updated reward\n",
    "\n",
    "    Returns:\n",
    "    V_new (array) : New value of each state\n",
    "    \"\"\"\n",
    "    expr_new = np.exp(r_new[agent.terminals] / agent._lambda)\n",
    "    Z_new = np.zeros(len(r_new))\n",
    "\n",
    "    Z_new[~agent.terminals] = agent.DR[~agent.terminals][:,~agent.terminals] @ agent.P @ expr_new\n",
    "    Z_new[agent.terminals] = expr_new\n",
    "    V_new = np.round(np.log(Z_new), 2)\n",
    "\n",
    "    return V_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearRL-TD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRL:\n",
    "    def __init__(self, env_name, alpha=0.1, beta=1, gamma=0.904, _lambda=1.0, epsilon=0.4, num_steps=25000, policy=\"random\", imp_samp=True):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.start_loc = self.env.unwrapped.start_loc\n",
    "        self.target_locs = self.env.unwrapped.target_locs\n",
    "        self.maze = self.env.unwrapped.maze\n",
    "        self.walls = self.env.unwrapped.get_walls()\n",
    "        self.size = self.maze.size - len(self.walls)   # Size of the state space is the = size of maze - number of blocked states\n",
    "        self.height, self.width = self.maze.shape\n",
    "        # self.target_locs = [self.target_loc]\n",
    "\n",
    "        # Create mapping and Transition matrix\n",
    "        self.mapping = create_mapping_nb(self.maze, self.walls)\n",
    "        self.reverse_mapping = {index: (i, j) for (i, j), index in self.mapping.items()}\n",
    "        self.T = get_transition_matrix(self.env, self.mapping)\n",
    "        \n",
    "\n",
    "        # Get terminal states\n",
    "        self.terminals = np.diag(self.T) == 1\n",
    "        # Calculate P = T_{NT}\n",
    "        self.P = self.T[~self.terminals][:,self.terminals]\n",
    "        # Set reward\n",
    "        self.reward_nt = -1   # Non-terminal state reward\n",
    "        self.reward_t = -1    # Terminal state reward\n",
    "        self.r = np.full(len(self.T), self.reward_nt)\n",
    "        self.r[self.terminals] = self.reward_t\n",
    "        self.expr_t = np.exp(self.r[self.terminals] / _lambda)\n",
    "        # Precalculate exp(r) for use with LinearRL equations\n",
    "        self.expr_nt = np.exp(self.reward_nt / _lambda)\n",
    "\n",
    "        # Params\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = self.expr_nt\n",
    "        self._lambda = _lambda\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.policy = policy\n",
    "        self.imp_samp = imp_samp\n",
    "\n",
    "        # Model\n",
    "        self.DR = self.get_DR()\n",
    "        self.Z = np.full(self.size, 0.01)\n",
    "\n",
    "        self.V = np.zeros(self.size)\n",
    "        self.one_hot = np.eye(self.size)\n",
    "\n",
    "    def get_states(self):\n",
    "        \"\"\"\n",
    "        Returns all non-blocked states as well as a mapping of each state (i,j) -> to an index (k)\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        index_mapping = {}\n",
    "        index = 0\n",
    "        for i in range(len(self.maze)):\n",
    "            for j in range(len(self.maze[i])):\n",
    "                if self.maze[i][j] in ['0', 'S', 'G']:\n",
    "                    states.append((i, j))\n",
    "                    index_mapping[(i, j)] = index\n",
    "                    index += 1\n",
    "\n",
    "        return states, index_mapping\n",
    "\n",
    "    def get_DR(self):\n",
    "        \"\"\"\n",
    "        Returns the DR initialization based on what decision policy we are using, values are filled with 0.01 if using softmax to avoid div by zero\n",
    "        \"\"\"\n",
    "        if self.policy == \"random\":\n",
    "            DR = np.eye(self.size)\n",
    "            DR[np.where(self.terminals)[0], np.where(self.terminals)[0]] = (1/(1-self.gamma))\n",
    "        \n",
    "        elif self.policy == \"softmax\":\n",
    "            DR = np.full((self.size, self.size), 0.01)\n",
    "            np.fill_diagonal(DR, 1)\n",
    "            DR[np.where(self.terminals)[0], np.where(self.terminals)[0]] = (1/(1-self.gamma))\n",
    "\n",
    "        return DR\n",
    "\n",
    "    def update_V(self):\n",
    "        self.Z[~self.terminals] = self.DR[~self.terminals][:,~self.terminals] @ self.P @ self.expr_t\n",
    "        self.Z[self.terminals] = self.expr_t\n",
    "        self.V = np.round(np.log(self.Z), 2)\n",
    "    \n",
    "    def importance_sampling(self, state, s_prob):\n",
    "        \"\"\"\n",
    "        Performs importance sampling P(x'|x)/u(x'|x). P(.) is the default policy, u(.) us the decision policy\n",
    "        \"\"\"\n",
    "        successor_states = self.env.unwrapped.get_successor_states(state)\n",
    "        p = 1/len(successor_states)\n",
    "        w = p/s_prob\n",
    "                \n",
    "        return w\n",
    "\n",
    "    def select_action(self, state, beta=0.5, target_loc=None):\n",
    "        \"\"\"\n",
    "        Action selection based on our policy\n",
    "        Options are: [random, softmax, egreedy, test]\n",
    "        \"\"\"\n",
    "        if self.policy == \"random\":\n",
    "            return self.env.unwrapped.random_action()\n",
    "        \n",
    "        elif self.policy == \"softmax\":\n",
    "            successor_states = self.env.unwrapped.get_successor_states(state)      # succesor_states = [(state, terminated), ...]\n",
    "            action_probs = np.full(self.env.action_space.n, 0.0)\n",
    "\n",
    "            v_sum = sum(\n",
    "                        np.exp((np.log(self.Z[self.mapping[(s[0][0],s[0][1])]] + 1e-20)) / self.beta) for s in successor_states\n",
    "                        )\n",
    "\n",
    "            # if we don't have enough info, random action\n",
    "            if v_sum == 0:\n",
    "                return self.env.unwrapped.random_action() \n",
    "\n",
    "            for action in self.env.unwrapped.get_available_actions(state):\n",
    "                direction = self.env.unwrapped._action_to_direction[action]\n",
    "                new_state = state + direction\n",
    "                \n",
    "                action_probs[action] = np.exp((np.log(self.Z[self.mapping[(new_state[0], new_state[1])]] + 1e-20)) / self.beta ) / v_sum\n",
    "\n",
    "            action = np.random.choice(self.env.action_space.n, p=action_probs)\n",
    "            s_prob = action_probs[action]\n",
    "\n",
    "            return action, s_prob\n",
    "    \n",
    "        elif self.policy == \"egreedy\":\n",
    "            if np.random.uniform(low=0, high=1) < self.epsilon:\n",
    "                return self.env.unwrapped.random_action()\n",
    "            else:\n",
    "                action_values = np.full(self.env.action_space.n, -np.inf)\n",
    "                for action in self.env.unwrapped.get_available_actions(state):\n",
    "                    direction = self.env.unwrapped._action_to_direction[action]\n",
    "                    new_state = state + direction\n",
    "\n",
    "                    if self.maze[new_state[0], new_state[1]] == \"1\":\n",
    "                        continue\n",
    "\n",
    "                    action_values[action] = round(np.log(self.Z[self.mapping[(new_state[0],new_state[1])]]), 2)\n",
    "\n",
    "                return np.argmax(action_values)\n",
    "            \n",
    "        elif self.policy == \"test\":\n",
    "            action_values = np.full(self.env.action_space.n, -np.inf)\n",
    "            for action in self.env.unwrapped.get_available_actions(state):\n",
    "                direction = self.env.unwrapped._action_to_direction[action]\n",
    "                new_state = state + direction\n",
    "\n",
    "                # Need this to make it work for now\n",
    "                if np.array_equal(new_state, target_loc):\n",
    "                    return action\n",
    "\n",
    "                if self.maze[new_state[0], new_state[1]] == \"1\":\n",
    "                    continue\n",
    "                action_values[action] = round(np.log(self.Z[self.mapping[(new_state[0],new_state[1])]]), 2)\n",
    "\n",
    "            return np.nanargmax(action_values)\n",
    "\n",
    "    def get_D_inv(self):\n",
    "        \"\"\"\n",
    "        Calculates the DR directly using matrix inversion, used for testing\n",
    "        \"\"\"\n",
    "        I = np.eye(self.size)\n",
    "        D_inv = np.linalg.inv(I-self.gamma*self.T)\n",
    "\n",
    "        return D_inv\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Agent explores the maze according to its decision policy and and updates its DR as it goes\n",
    "        \"\"\"\n",
    "        print(f\"Decision Policy: {self.policy}, Number of Iterations: {self.num_steps}, lr={self.alpha}, temperature={self.beta}\")\n",
    "        self.env.reset()\n",
    "\n",
    "        # D_inv_1 = self.get_D_inv()\n",
    "        # D_inv_2 = np.linalg.inv(np.diag(np.exp(-self.r))-self.T)\n",
    "\n",
    "        # Iterate through number of steps\n",
    "        for i in range(self.num_steps):\n",
    "            # Current state\n",
    "            state = self.env.unwrapped.agent_loc\n",
    "            state_idx = self.mapping[(state[0], state[1])]\n",
    "\n",
    "            # Choose action\n",
    "            if self.policy == \"softmax\":\n",
    "                action, s_prob = self.select_action(state)\n",
    "            else:\n",
    "                action = self.select_action(state, self.policy)\n",
    "        \n",
    "            # Take action\n",
    "            obs, _, done, _, _ = self.env.step(action)\n",
    "\n",
    "            # Unpack observation to get new state\n",
    "            next_state = obs[\"agent\"]\n",
    "            next_state_idx = self.mapping[(next_state[0], next_state[1])]\n",
    "\n",
    "            # Importance sampling\n",
    "            if self.policy == \"softmax\":\n",
    "                w = self.importance_sampling(state, s_prob)\n",
    "                w = 1 if np.isnan(w) or w == 0 else w\n",
    "            else:\n",
    "                w = 1\n",
    "            \n",
    "            ## Update default representation\n",
    "            target = self.one_hot[state_idx] + self.gamma * self.DR[next_state_idx]\n",
    "            # If we are using importance sampling\n",
    "            if self.imp_samp:\n",
    "                self.DR[state_idx] = (1 - self.alpha) * self.DR[state_idx] + self.alpha * target * w\n",
    "            else:\n",
    "                self.DR[state_idx] = (1 - self.alpha) * self.DR[state_idx] + self.alpha * target\n",
    "\n",
    "            ## Update Z-Values\n",
    "            self.Z = self.DR[:,~self.terminals] @ self.P @ self.expr_t\n",
    "\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "                continue\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "        # Update DR at terminal state\n",
    "        self.Z[self.terminals] = np.exp(self.r[self.terminals] / self._lambda)\n",
    "        self.V = np.round(np.log(self.Z), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D_inv agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.95 -6.36 -4.7  -3.04 -1.  ]\n",
      " [-8.66  -inf  -inf -4.42 -3.05]\n",
      " [-7.43  -inf  -inf -5.67 -5.01]\n",
      " [-5.79 -4.21 -2.65  -inf -5.59]\n",
      " [-4.67 -3.03 -1.   -2.66 -4.3 ]]\n"
     ]
    }
   ],
   "source": [
    "# Agent to be used with D_inv\n",
    "agent = LinearRL(env_name=\"simple-5x5-2\", _lambda=1.0, alpha=0.001, beta=1.0, num_steps=500000, policy=\"softmax\", imp_samp=True)\n",
    "\n",
    "D_inv = np.linalg.inv(np.diag(np.exp(-agent.r / agent._lambda)) - agent.T)\n",
    "\n",
    "agent.DR = D_inv\n",
    "agent.update_V()\n",
    "maze_values = get_full_maze_values(agent)\n",
    "print(maze_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent with importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "agent_with_imp = LinearRL(env_name=\"simple-5x5-2\", _lambda=1.0, alpha=0.001, beta=0.2, num_steps=500000, policy=\"softmax\", imp_samp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Policy: softmax, Number of Iterations: 500000, lr=0.001, temperature=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abizzle/anaconda3/envs/gym/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:181: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/Users/abizzle/anaconda3/envs/gym/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/Users/abizzle/anaconda3/envs/gym/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "agent_with_imp.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maze:\n",
      "[['S' '0' '0' '0' 'G']\n",
      " ['0' '1' '1' '0' '0']\n",
      " ['0' '1' '1' '0' '0']\n",
      " ['0' '0' '0' '1' '0']\n",
      " ['0' '0' 'G' '0' '0']]\n",
      "-------------------------------------\n",
      "Values:\n",
      "[[-5.25 -5.06 -3.92 -1.77 -1.  ]\n",
      " [-5.12  -inf  -inf -3.65 -2.43]\n",
      " [-5.05  -inf  -inf -4.91 -4.79]\n",
      " [-4.99 -3.57 -1.65  -inf -4.91]\n",
      " [-4.66 -2.44 -1.   -1.66 -4.91]]\n"
     ]
    }
   ],
   "source": [
    "# Print out the values to see what it learned\n",
    "maze_values = get_full_maze_values(agent_with_imp)\n",
    "print(\"Maze:\")\n",
    "print(agent.maze)\n",
    "print(\"-------------------------------------\")\n",
    "print(\"Values:\")\n",
    "print(maze_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent without importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_no_imp = LinearRL(env_name=\"simple-5x5-2\", _lambda=1.0, alpha=0.001, beta=0.2, num_steps=500000, policy=\"softmax\", imp_samp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Policy: softmax, Number of Iterations: 500000, lr=0.001, temperature=0.2\n"
     ]
    }
   ],
   "source": [
    "# Train agent without importance sampling\n",
    "agent_no_imp.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maze:\n",
      "[['S' '0' '0' '0' 'G']\n",
      " ['0' '1' '1' '0' '0']\n",
      " ['0' '1' '1' '0' '0']\n",
      " ['0' '0' '0' '1' '0']\n",
      " ['0' '0' 'G' '0' '0']]\n",
      "-------------------------------------\n",
      "Values:\n",
      "[[-5.01 -4.94 -2.95 -1.95 -1.  ]\n",
      " [-4.97  -inf  -inf -2.95 -1.95]\n",
      " [-4.94  -inf  -inf -4.91 -4.75]\n",
      " [-4.93 -2.6  -1.55  -inf -4.91]\n",
      " [-4.8  -1.9  -1.   -1.66 -4.91]]\n"
     ]
    }
   ],
   "source": [
    "# Print out the values to see what it learned\n",
    "maze_values = get_full_maze_values(agent_no_imp)\n",
    "print(\"Maze:\")\n",
    "print(agent.maze)\n",
    "print(\"-------------------------------------\")\n",
    "print(\"Values:\")\n",
    "print(maze_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Agents DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "loc = 17\n",
    "r_new = np.full(20, -1)\n",
    "r_new[loc] = 1\n",
    "print(r_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update agent with D_inv, to verify working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_new = policy_reval(agent=agent, r_new=r_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.63 -6.34 -4.7  -3.04 -1.  ]\n",
      " [-7.03  -inf  -inf -4.41 -3.04]\n",
      " [-5.45  -inf  -inf -5.56 -4.74]\n",
      " [-3.79 -2.21 -0.65  -inf -3.92]\n",
      " [-2.67 -1.03  1.   -0.66 -2.31]]\n"
     ]
    }
   ],
   "source": [
    "agent.V = V_new\n",
    "maze_values = get_full_maze_values(agent)\n",
    "print(maze_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update agent with importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_new = policy_reval(agent=agent_with_imp, r_new=r_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.59 -3.47 -3.9  -1.73 -1.  ]\n",
      " [-3.46  -inf  -inf -3.62 -2.39]\n",
      " [-3.39  -inf  -inf -3.25 -3.32]\n",
      " [-3.31 -1.57  0.35  -inf -3.25]\n",
      " [-2.89 -0.44  1.    0.33 -3.25]]\n"
     ]
    }
   ],
   "source": [
    "agent_with_imp.V = V_new\n",
    "maze_values = get_full_maze_values(agent_with_imp)\n",
    "print(maze_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update agent without importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_new = policy_reval(agent=agent_no_imp, r_new=r_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.35 -3.3  -2.95 -1.95 -1.  ]\n",
      " [-3.31  -inf  -inf -2.95 -1.95]\n",
      " [-3.28  -inf  -inf -3.25 -3.24]\n",
      " [-3.27 -0.6   0.45  -inf -3.25]\n",
      " [-3.09  0.1   1.    0.33 -3.25]]\n"
     ]
    }
   ],
   "source": [
    "agent_no_imp.V = V_new\n",
    "maze_values = get_full_maze_values(agent_no_imp)\n",
    "print(maze_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
