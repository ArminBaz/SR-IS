{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import matplotlib.animation as manimation\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import gym_env\n",
    "import utils\n",
    "from utils import create_mapping, get_transition_matrix, create_mapping_nb, get_full_maze_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_terminal_reward(agent, loc, r):\n",
    "    \"\"\"\n",
    "    Update the reward for the terminal state of the agent according to loc\n",
    "\n",
    "    Args:\n",
    "    agent (LinearRL class) : The LinearRL agent\n",
    "    loc (int) : The location to change (1 or 2)\n",
    "    r (float) : The new reward to change r[loc] to\n",
    "    \"\"\"\n",
    "    # Get location of reward and change\n",
    "    r_loc = np.argwhere(agent.terminals)[loc]\n",
    "    agent.r[r_loc] = r\n",
    "    # Update expr_t inside of the agent\n",
    "    agent.expr_t = np.exp(agent.r[agent.terminals] / agent._lambda)\n",
    "\n",
    "def policy_reval(agent, r_new):\n",
    "    \"\"\"\n",
    "    The New environment is the same as the old one except we \n",
    "    \n",
    "    Args:\n",
    "    agent (LinearRL class) : The LinearRL agent\n",
    "    r_new (array) : Updated reward\n",
    "\n",
    "    Returns:\n",
    "    V_new (array) : New value of each state\n",
    "    \"\"\"\n",
    "    expr_new = np.exp(r_new[agent.terminals] / agent._lambda)\n",
    "    Z_new = np.zeros(len(r_new))\n",
    "\n",
    "    Z_new[~agent.terminals] = agent.DR[~agent.terminals][:,~agent.terminals] @ agent.P @ expr_new\n",
    "    Z_new[agent.terminals] = expr_new\n",
    "    V_new = np.round(np.log(Z_new), 2)\n",
    "\n",
    "    return V_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearRL-TD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRL:\n",
    "    def __init__(self, env_name, alpha=0.1, beta=1, gamma=0.904, _lambda=1.0, epsilon=0.4, num_steps=25000, policy=\"random\", imp_samp=True):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.start_loc = self.env.unwrapped.start_loc\n",
    "        self.target_locs = self.env.unwrapped.target_locs\n",
    "        self.maze = self.env.unwrapped.maze\n",
    "        self.walls = self.env.unwrapped.get_walls()\n",
    "        self.size = self.maze.size - len(self.walls)   # Size of the state space is the = size of maze - number of blocked states\n",
    "        self.height, self.width = self.maze.shape\n",
    "        # self.target_locs = [self.target_loc]\n",
    "\n",
    "        # Create mapping and Transition matrix\n",
    "        self.mapping = create_mapping_nb(self.maze, self.walls)\n",
    "        self.reverse_mapping = {index: (i, j) for (i, j), index in self.mapping.items()}\n",
    "        self.T = get_transition_matrix(self.env, self.mapping)\n",
    "        \n",
    "\n",
    "        # Get terminal states\n",
    "        self.terminals = np.diag(self.T) == 1\n",
    "        # Calculate P = T_{NT}\n",
    "        self.P = self.T[~self.terminals][:,self.terminals]\n",
    "        # Set reward\n",
    "        self.reward_nt = -1   # Non-terminal state reward\n",
    "        self.reward_t = -1    # Terminal state reward\n",
    "        self.r = np.full(len(self.T), self.reward_nt)\n",
    "        self.r[self.terminals] = self.reward_t\n",
    "        self.expr_t = np.exp(self.r[self.terminals] / _lambda)\n",
    "        # Precalculate exp(r) for use with LinearRL equations\n",
    "        self.expr_nt = np.exp(self.reward_nt / _lambda)\n",
    "\n",
    "        # Params\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = self.expr_nt\n",
    "        self._lambda = _lambda\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.policy = policy\n",
    "        self.imp_samp = imp_samp\n",
    "\n",
    "        # Model\n",
    "        self.DR = self.get_DR()\n",
    "        self.Z = np.full(self.size, 0.01)\n",
    "\n",
    "        self.V = np.zeros(self.size)\n",
    "        self.one_hot = np.eye(self.size)\n",
    "\n",
    "    def get_states(self):\n",
    "        \"\"\"\n",
    "        Returns all non-blocked states as well as a mapping of each state (i,j) -> to an index (k)\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        index_mapping = {}\n",
    "        index = 0\n",
    "        for i in range(len(self.maze)):\n",
    "            for j in range(len(self.maze[i])):\n",
    "                if self.maze[i][j] in ['0', 'S', 'G']:\n",
    "                    states.append((i, j))\n",
    "                    index_mapping[(i, j)] = index\n",
    "                    index += 1\n",
    "\n",
    "        return states, index_mapping\n",
    "\n",
    "    def get_DR(self):\n",
    "        \"\"\"\n",
    "        Returns the DR initialization based on what decision policy we are using, values are filled with 0.01 if using softmax to avoid div by zero\n",
    "        \"\"\"\n",
    "        if self.policy == \"random\":\n",
    "            DR = np.eye(self.size)\n",
    "            DR[np.where(self.terminals)[0], np.where(self.terminals)[0]] = (1/(1-self.gamma))\n",
    "        \n",
    "        elif self.policy == \"softmax\":\n",
    "            DR = np.full((self.size, self.size), 0.01)\n",
    "            np.fill_diagonal(DR, 1)\n",
    "            DR[np.where(self.terminals)[0], np.where(self.terminals)[0]] = (1/(1-self.gamma))\n",
    "\n",
    "        return DR\n",
    "\n",
    "    def update_V(self):\n",
    "        self.Z[~self.terminals] = self.DR[~self.terminals][:,~self.terminals] @ self.P @ self.expr_t\n",
    "        self.Z[self.terminals] = self.expr_t\n",
    "        self.V = np.round(np.log(self.Z), 2)\n",
    "    \n",
    "    def importance_sampling(self, state, s_prob):\n",
    "        \"\"\"\n",
    "        Performs importance sampling P(x'|x)/u(x'|x). P(.) is the default policy, u(.) us the decision policy\n",
    "        \"\"\"\n",
    "        successor_states = self.env.unwrapped.get_successor_states(state)\n",
    "        p = 1/len(successor_states)\n",
    "        w = p/s_prob\n",
    "                \n",
    "        return w\n",
    "\n",
    "    def select_action(self, state, beta=0.5, target_loc=None):\n",
    "        \"\"\"\n",
    "        Action selection based on our policy\n",
    "        Options are: [random, softmax, egreedy, test]\n",
    "        \"\"\"\n",
    "        if self.policy == \"random\":\n",
    "            return self.env.unwrapped.random_action()\n",
    "        \n",
    "        elif self.policy == \"softmax\":\n",
    "            successor_states = self.env.unwrapped.get_successor_states(state)      # succesor_states = [(state, terminated), ...]\n",
    "            action_probs = np.full(self.env.action_space.n, 0.0)\n",
    "\n",
    "            v_sum = sum(\n",
    "                        np.exp((np.log(self.Z[self.mapping[(s[0][0],s[0][1])]] + 1e-20)) / self.beta) for s in successor_states\n",
    "                        )\n",
    "\n",
    "            # if we don't have enough info, random action\n",
    "            if v_sum == 0:\n",
    "                return self.env.unwrapped.random_action() \n",
    "\n",
    "            for action in self.env.unwrapped.get_available_actions(state):\n",
    "                direction = self.env.unwrapped._action_to_direction[action]\n",
    "                new_state = state + direction\n",
    "                \n",
    "                action_probs[action] = np.exp((np.log(self.Z[self.mapping[(new_state[0], new_state[1])]] + 1e-20)) / self.beta ) / v_sum\n",
    "\n",
    "            action = np.random.choice(self.env.action_space.n, p=action_probs)\n",
    "            s_prob = action_probs[action]\n",
    "\n",
    "            return action, s_prob\n",
    "    \n",
    "        elif self.policy == \"egreedy\":\n",
    "            if np.random.uniform(low=0, high=1) < self.epsilon:\n",
    "                return self.env.unwrapped.random_action()\n",
    "            else:\n",
    "                action_values = np.full(self.env.action_space.n, -np.inf)\n",
    "                for action in self.env.unwrapped.get_available_actions(state):\n",
    "                    direction = self.env.unwrapped._action_to_direction[action]\n",
    "                    new_state = state + direction\n",
    "\n",
    "                    if self.maze[new_state[0], new_state[1]] == \"1\":\n",
    "                        continue\n",
    "\n",
    "                    action_values[action] = round(np.log(self.Z[self.mapping[(new_state[0],new_state[1])]]), 2)\n",
    "\n",
    "                return np.argmax(action_values)\n",
    "            \n",
    "        elif self.policy == \"test\":\n",
    "            action_values = np.full(self.env.action_space.n, -np.inf)\n",
    "            for action in self.env.unwrapped.get_available_actions(state):\n",
    "                direction = self.env.unwrapped._action_to_direction[action]\n",
    "                new_state = state + direction\n",
    "\n",
    "                # Need this to make it work for now\n",
    "                if np.array_equal(new_state, target_loc):\n",
    "                    return action\n",
    "\n",
    "                if self.maze[new_state[0], new_state[1]] == \"1\":\n",
    "                    continue\n",
    "                action_values[action] = round(np.log(self.Z[self.mapping[(new_state[0],new_state[1])]]), 2)\n",
    "\n",
    "            return np.nanargmax(action_values)\n",
    "\n",
    "    def get_D_inv(self):\n",
    "        \"\"\"\n",
    "        Calculates the DR directly using matrix inversion, used for testing\n",
    "        \"\"\"\n",
    "        I = np.eye(self.size)\n",
    "        D_inv = np.linalg.inv(I-self.gamma*self.T)\n",
    "\n",
    "        return D_inv\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Agent explores the maze according to its decision policy and and updates its DR as it goes\n",
    "        \"\"\"\n",
    "        print(f\"Decision Policy: {self.policy}, Number of Iterations: {self.num_steps}, lr={self.alpha}, temperature={self.beta}, importance sampling={self.imp_samp}\")\n",
    "        self.env.reset()\n",
    "\n",
    "        # D_inv_1 = self.get_D_inv()\n",
    "        # D_inv_2 = np.linalg.inv(np.diag(np.exp(-self.r))-self.T)\n",
    "\n",
    "        # Iterate through number of steps\n",
    "        for i in range(self.num_steps):\n",
    "            # Current state\n",
    "            state = self.env.unwrapped.agent_loc\n",
    "            state_idx = self.mapping[(state[0], state[1])]\n",
    "\n",
    "            # Choose action\n",
    "            if self.policy == \"softmax\":\n",
    "                action, s_prob = self.select_action(state)\n",
    "            else:\n",
    "                action = self.select_action(state, self.policy)\n",
    "        \n",
    "            # Take action\n",
    "            obs, _, done, _, _ = self.env.step(action)\n",
    "\n",
    "            # Unpack observation to get new state\n",
    "            next_state = obs[\"agent\"]\n",
    "            next_state_idx = self.mapping[(next_state[0], next_state[1])]\n",
    "\n",
    "            # Importance sampling\n",
    "            if self.policy == \"softmax\":\n",
    "                w = self.importance_sampling(state, s_prob)\n",
    "                w = 1 if np.isnan(w) or w == 0 else w\n",
    "            else:\n",
    "                w = 1\n",
    "            \n",
    "            ## Update default representation\n",
    "            target = self.one_hot[state_idx] + self.gamma * self.DR[next_state_idx]\n",
    "            # If we are using importance sampling\n",
    "            if self.imp_samp:\n",
    "                self.DR[state_idx] = (1 - self.alpha) * self.DR[state_idx] + self.alpha * target * w\n",
    "            else:\n",
    "                self.DR[state_idx] = (1 - self.alpha) * self.DR[state_idx] + self.alpha * target\n",
    "\n",
    "            ## Update Z-Values\n",
    "            self.Z = self.DR[:,~self.terminals] @ self.P @ self.expr_t\n",
    "\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "                continue\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "        # Update DR at terminal state\n",
    "        self.Z[self.terminals] = np.exp(self.r[self.terminals] / self._lambda)\n",
    "        self.V = np.round(np.log(self.Z), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mazes = [\"simple-5x5-2\", \"simple-7x7-2\"]\n",
    "maze_name = mazes[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D_inv agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -9.6   -7.95  -6.29  -4.63  -3.26  -2.29  -2.85]\n",
      " [-11.22   -inf   -inf  -3.24  -1.6   -0.32  -1.54]\n",
      " [-12.07   -inf   -inf   -inf   0.33   2.    -0.04]\n",
      " [-10.94  -9.72   -inf   -inf   -inf  -0.05  -1.43]\n",
      " [ -9.45  -8.08  -6.28  -4.96   -inf  -2.09  -3.09]\n",
      " [ -8.05  -6.61  -4.84  -3.32  -4.42  -4.27  -4.87]\n",
      " [ -6.75  -5.11  -3.06  -1.    -3.05  -4.84  -5.86]]\n"
     ]
    }
   ],
   "source": [
    "# Agent to be used with D_inv\n",
    "agent = LinearRL(env_name=maze_name, _lambda=1.0, alpha=0.001, beta=1.0, num_steps=500000, policy=\"softmax\", imp_samp=True)\n",
    "# Make the reward for the first terminal state higher than the second to bias the DR towards that terminal state\n",
    "update_terminal_reward(agent, loc=0, r=2)\n",
    "\n",
    "D_inv = np.linalg.inv(np.diag(np.exp(-agent.r / agent._lambda)) - agent.T)\n",
    "\n",
    "agent.DR = D_inv\n",
    "agent.update_V()\n",
    "maze_values = get_full_maze_values(agent)\n",
    "print(maze_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the second terminal state to make it double that of the first\n",
    "update_terminal_reward(agent, loc=1, r=6)\n",
    "V_new = policy_reval(agent=agent, r_new=agent.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.52 -7.88 -6.29 -4.63 -3.26 -2.29 -2.85]\n",
      " [-7.25  -inf  -inf -3.24 -1.6  -0.32 -1.54]\n",
      " [-5.61  -inf  -inf  -inf  0.33  2.   -0.04]\n",
      " [-3.96 -2.72  -inf  -inf  -inf -0.02 -1.37]\n",
      " [-2.45 -1.09  0.72  2.04  -inf -1.06 -2.11]\n",
      " [-1.05  0.39  2.16  3.68  2.43  0.55 -0.95]\n",
      " [ 0.25  1.89  3.94  6.    3.95  1.9   0.27]]\n"
     ]
    }
   ],
   "source": [
    "agent.V = V_new\n",
    "maze_values = get_full_maze_values(agent)\n",
    "print(maze_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent with importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "agent_with_imp = LinearRL(env_name=\"simple-7x7-2\", _lambda=1.0, alpha=0.001, beta=0.5, num_steps=500000, policy=\"softmax\", imp_samp=True)\n",
    "update_terminal_reward(agent_with_imp, loc=0, r=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Policy: softmax, Number of Iterations: 500000, lr=0.001, temperature=0.5\n"
     ]
    }
   ],
   "source": [
    "# Train the agent with importance sampling\n",
    "agent_with_imp.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.07 -6.28 -5.13 -3.65 -2.28 -1.32 -2.2 ]\n",
      " [-6.47  -inf  -inf -2.25 -0.66  0.62 -0.55]\n",
      " [-5.93  -inf  -inf  -inf  1.35  2.    0.96]\n",
      " [-5.29 -4.56  -inf  -inf  -inf  0.9  -0.44]\n",
      " [-4.51 -4.4  -3.53 -2.99  -inf -1.08 -2.19]\n",
      " [-4.01 -3.77 -3.22 -2.07 -2.57 -2.35 -2.28]\n",
      " [-3.45 -3.35 -1.88 -1.   -1.65 -2.38 -2.28]]\n"
     ]
    }
   ],
   "source": [
    "# Print out the values to see what it learned\n",
    "maze_values = get_full_maze_values(agent_with_imp)\n",
    "print(maze_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_terminal_reward(agent_with_imp, loc=1, r=6)\n",
    "V_new = policy_reval(agent=agent_with_imp, r_new=agent_with_imp.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.6  -3.01 -2.32 -1.47 -0.37  0.23  1.28]\n",
      " [-2.91  -inf  -inf -0.8  -0.4   0.84  0.57]\n",
      " [-2.36  -inf  -inf  -inf  1.53  2.    1.18]\n",
      " [-1.68 -0.88  -inf  -inf  -inf  1.1   0.26]\n",
      " [-0.8  -0.18  1.41  2.82  -inf  0.02  1.28]\n",
      " [ 0.01  1.23  3.09  4.66  2.99  1.23  1.28]\n",
      " [ 1.13  2.84  4.97  6.    4.92  2.    1.29]]\n"
     ]
    }
   ],
   "source": [
    "agent_with_imp.V = V_new\n",
    "maze_values = get_full_maze_values(agent_with_imp)\n",
    "print(maze_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent without importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_no_imp = LinearRL(env_name=\"simple-7x7-2\", _lambda=1.0, alpha=0.001, beta=0.5, num_steps=500000, policy=\"softmax\", imp_samp=False)\n",
    "update_terminal_reward(agent_no_imp, loc=0, r=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Policy: softmax, Number of Iterations: 500000, lr=0.001, temperature=0.5\n"
     ]
    }
   ],
   "source": [
    "# Train agent without importance sampling\n",
    "agent_no_imp.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.34 -3.78 -3.24 -2.32 -1.06 -0.21 -2.15]\n",
      " [-3.92  -inf  -inf -0.97  0.31  0.82 -0.04]\n",
      " [-3.69  -inf  -inf  -inf  1.43  2.    1.04]\n",
      " [-3.49 -3.09  -inf  -inf  -inf  1.05  0.04]\n",
      " [-3.11 -3.11 -2.66 -2.45  -inf  0.04 -2.09]\n",
      " [-2.89 -2.81 -2.55 -1.86 -2.33 -2.15 -2.23]\n",
      " [-2.6  -2.54 -1.73 -1.   -1.54 -2.26 -2.24]]\n"
     ]
    }
   ],
   "source": [
    "# Print out the values to see what it learned\n",
    "maze_values = get_full_maze_values(agent_no_imp)\n",
    "print(maze_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_terminal_reward(agent_no_imp, loc=1, r=6)\n",
    "V_new = policy_reval(agent=agent_no_imp, r_new=agent_no_imp.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.79 -0.25  0.11  0.37  0.39 -0.17  1.3 ]\n",
      " [-0.36  -inf  -inf  0.4   0.32  0.82 -0.04]\n",
      " [-0.13  -inf  -inf  -inf  1.44  2.    1.04]\n",
      " [ 0.07  0.48  -inf  -inf  -inf  1.05  0.05]\n",
      " [ 0.46  0.54  1.39  2.71  -inf  0.05  1.28]\n",
      " [ 0.74  1.46  3.26  4.65  2.68  1.28  1.33]\n",
      " [ 1.37  3.3   4.93  6.    4.92  2.02  1.33]]\n"
     ]
    }
   ],
   "source": [
    "agent_no_imp.V = V_new\n",
    "maze_values = get_full_maze_values(agent_no_imp)\n",
    "print(maze_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
