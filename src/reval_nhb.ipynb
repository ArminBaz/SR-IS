{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import matplotlib.animation as manimation\n",
    "\n",
    "import gym_env\n",
    "from models import LinearRL_NHB, SR_NHB\n",
    "from utils import policy_reval, woodbury\n",
    "from utils_render import plot_nhb_decisions, create_bar_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for NumPy\n",
    "seed = 42\n",
    "np.random.seed(42)\n",
    "\n",
    "# Save dir\n",
    "save_dir = os.path.join('..', 'figures/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting\n",
    "prob_locs = [1, 2]\n",
    "colors = [3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters for individual plots to make results more similar to NHB\n",
    "# This produces 0.46 and 0.65 = 0.19\n",
    "# lambd = 10\n",
    "# alpha = 0.1\n",
    "# alpha_SR = 0.2\n",
    "# beta = 0.9\n",
    "# num_steps = 115\n",
    "# num_iterations = 400\n",
    "\n",
    "## Hyperparameters for plots all together to show a regular agent\n",
    "# This produces 0.75 and 0.90 = 0.15\n",
    "lambd = 10\n",
    "alpha = 0.15\n",
    "alpha_SR = 0.15\n",
    "beta = 1.0\n",
    "num_steps = 250\n",
    "num_iterations = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Revaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_complete = {\"train\":np.array([0,0]), \"test\":np.array([0,0])}\n",
    "for i in range(num_iterations):\n",
    "    # Define agent\n",
    "    agent_complete =  LinearRL_NHB(_lambda=lambd, alpha=alpha, beta=beta, num_steps=num_steps, policy=\"softmax\", imp_samp=True, exp_type=\"policy_reval\")\n",
    "    # Get optimal DR and update V and Z Values\n",
    "    D_inv = agent_complete.get_D_inv()\n",
    "    agent_complete.DR = D_inv\n",
    "    agent_complete.update_Z()\n",
    "    agent_complete.update_V()\n",
    "    dec_train = np.argmax([agent_complete.V[1], agent_complete.V[2]])\n",
    "    choices_complete[\"train\"][dec_train] += 1\n",
    "\n",
    "    # Update terminal state values\n",
    "    agent_complete.update_exp()\n",
    "    V_new, Z_new = policy_reval(agent_complete)\n",
    "    dec_test = np.argmax([V_new[1], V_new[2]])\n",
    "    choices_complete[\"test\"][dec_test] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_with = {\"train\":np.array([0,0]), \"test\":np.array([0,0])}\n",
    "choices_without = {\"train\":np.array([0,0]), \"test\":np.array([0,0])}\n",
    "choices_sr = {\"train\":np.array([0,0]), \"test\":np.array([0,0])}\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Define agents\n",
    "    agent_with_imp = LinearRL_NHB(_lambda=lambd, alpha=alpha, beta=beta, num_steps=num_steps, policy=\"softmax\", imp_samp=True, exp_type=\"policy_reval\")\n",
    "    agent_without_imp = LinearRL_NHB(_lambda=lambd, alpha=alpha, beta=beta, num_steps=num_steps, policy=\"softmax\", imp_samp=False, exp_type=\"policy_reval\")\n",
    "    agent_SR = SR_NHB(alpha=alpha_SR, beta=10.0, num_steps=num_steps, policy=\"softmax\", exp_type=\"policy_reval\")\n",
    "\n",
    "    # Have the agents learn the environment\n",
    "    agent_with_imp.learn(seed=i)\n",
    "    agent_without_imp.learn(seed=i)\n",
    "    agent_SR.learn(seed=i)\n",
    "    \n",
    "    ## SR\n",
    "    # Original training policy\n",
    "    dec_train = np.argmax([agent_SR.V[1], agent_SR.V[2]])\n",
    "    choices_sr[\"train\"][dec_train] += 1\n",
    "    # Policy revaluation\n",
    "    agent_SR.update_exp()\n",
    "    agent_SR.update_V()\n",
    "    # New policy\n",
    "    dec_test = np.argmax([agent_SR.V[1], agent_SR.V[2]])\n",
    "    choices_sr[\"test\"][dec_test] += 1\n",
    "\n",
    "\n",
    "    ## With importance sampling\n",
    "    # Original training policy\n",
    "    dec_train = np.argmax([agent_with_imp.V[1], agent_with_imp.V[2]])\n",
    "    choices_with[\"train\"][dec_train] += 1\n",
    "    # Policy revaluation\n",
    "    agent_with_imp.update_exp()\n",
    "    V_new, Z_new = policy_reval(agent_with_imp)\n",
    "    # New policy\n",
    "    dec_test = np.argmax([V_new[1], V_new[2]])\n",
    "    choices_with[\"test\"][dec_test] += 1\n",
    "\n",
    "    ## Without importance sampling\n",
    "    # Original training policy\n",
    "    dec_train = np.argmax([agent_without_imp.V[1], agent_without_imp.V[2]])\n",
    "    choices_without[\"train\"][dec_train] += 1\n",
    "    # Policy revaluation\n",
    "    agent_without_imp.update_exp()\n",
    "    V_new, Z_new = policy_reval(agent_without_imp)\n",
    "    # New policy\n",
    "    dec_test = np.argmax([V_new[1], V_new[2]])\n",
    "    choices_without[\"test\"][dec_test] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 400]\n",
      "selected state 1: 0.755\n",
      "\n",
      "{'train': array([  1, 399]), 'test': array([182, 218])}\n",
      "selected state 1: 0.455\n"
     ]
    }
   ],
   "source": [
    "print(choices_with[\"train\"])\n",
    "print(f\"selected state 1: {choices_with['test'][0]/num_iterations}\\n\")\n",
    "print(choices_without)\n",
    "print(f\"selected state 1: {choices_without['test'][0]/num_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Selection Probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_policy_complete = choices_complete[\"test\"] / num_iterations\n",
    "p_comp = choices_complete[\"test\"][1] / num_iterations\n",
    "std_policy_complete = np.sqrt((p_comp * (1 - p_comp)) / num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_policy_sr = choices_sr[\"test\"] / num_iterations\n",
    "p_sr = choices_sr[\"test\"][1] / num_iterations\n",
    "std_policy_sr = np.sqrt((p_sr * (1 - p_sr)) / num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_policy_with = choices_with[\"test\"] / num_iterations\n",
    "\n",
    "p_test = choices_with[\"test\"][1] / num_iterations\n",
    "std_policy_with = np.sqrt((p_test * (1 - p_test)) / num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_policy_without = choices_without[\"test\"] / num_iterations\n",
    "\n",
    "p_test = choices_without[\"test\"][1] / num_iterations\n",
    "std_policy_without = np.sqrt((p_test * (1 - p_test)) / num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Revaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_complete = {\"train\":np.array([0,0]), \"test\":np.array([0,0])}\n",
    "for i in range(num_iterations):\n",
    "    # Define agent\n",
    "    agent_complete =  LinearRL_NHB(_lambda=lambd, alpha=alpha, beta=beta, num_steps=num_steps, policy=\"softmax\", imp_samp=True, exp_type=\"reward_reval\")\n",
    "    # Get optimal DR and update V and Z Values\n",
    "    D_inv = agent_complete.get_D_inv()\n",
    "    agent_complete.DR = D_inv\n",
    "    agent_complete.update_Z()\n",
    "    agent_complete.update_V()\n",
    "    dec_train = np.argmax([agent_complete.V[1], agent_complete.V[2]])\n",
    "    choices_complete[\"train\"][dec_train] += 1\n",
    "\n",
    "    # Update terminal state values\n",
    "    agent_complete.update_exp()\n",
    "    V_new, Z_new = policy_reval(agent_complete)\n",
    "    dec_test = np.argmax([V_new[1], V_new[2]])\n",
    "    choices_complete[\"test\"][dec_test] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_with = {\"train\":np.array([0,0]), \"test\":np.array([0,0])}\n",
    "choices_without = {\"train\":np.array([0,0]), \"test\":np.array([0,0])}\n",
    "choices_sr = {\"train\":np.array([0,0]), \"test\":np.array([0,0])}\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Define agents\n",
    "    agent_with_imp = LinearRL_NHB(_lambda=lambd, alpha=alpha, beta=beta, num_steps=num_steps, policy=\"softmax\", imp_samp=True, exp_type=\"reward_reval\")\n",
    "    agent_without_imp = LinearRL_NHB(_lambda=lambd, alpha=alpha, beta=beta, num_steps=num_steps, policy=\"softmax\", imp_samp=False, exp_type=\"reward_reval\")\n",
    "    agent_SR = SR_NHB(alpha=alpha_SR, beta=10.0, num_steps=num_steps, policy=\"softmax\", exp_type=\"reward_reval\")\n",
    "\n",
    "    # Have the agents learn the environment\n",
    "    agent_with_imp.learn(seed=i)\n",
    "    agent_without_imp.learn(seed=i)\n",
    "    agent_SR.learn(seed=i)\n",
    "\n",
    "    ## SR\n",
    "    # Original training policy\n",
    "    dec_train = np.argmax([agent_SR.V[1], agent_SR.V[2]])\n",
    "    choices_sr[\"train\"][dec_train] += 1\n",
    "    # Policy revaluation\n",
    "    agent_SR.update_exp()\n",
    "    agent_SR.update_V()\n",
    "    # New policy\n",
    "    dec_test = np.argmax([agent_SR.V[1], agent_SR.V[2]])\n",
    "    choices_sr[\"test\"][dec_test] += 1\n",
    "    \n",
    "    ## With importance sampling\n",
    "    # Original training policy\n",
    "    dec_train = np.argmax([agent_with_imp.V[1], agent_with_imp.V[2]])\n",
    "    choices_with[\"train\"][dec_train] += 1\n",
    "    # Policy revaluation\n",
    "    agent_with_imp.update_exp()\n",
    "    V_new, Z_new = policy_reval(agent_with_imp)\n",
    "    # New policy\n",
    "    dec_test = np.argmax([V_new[1], V_new[2]])\n",
    "    choices_with[\"test\"][dec_test] += 1\n",
    "\n",
    "    ## Without importance sampling\n",
    "    # Original training policy\n",
    "    dec_train = np.argmax([agent_without_imp.V[1], agent_without_imp.V[2]])\n",
    "    choices_without[\"train\"][dec_train] += 1\n",
    "    # Policy revaluation\n",
    "    agent_without_imp.update_exp()\n",
    "    V_new, Z_new = policy_reval(agent_without_imp)\n",
    "    # New policy\n",
    "    dec_test = np.argmax([V_new[1], V_new[2]])\n",
    "    choices_without[\"test\"][dec_test] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 400]\n",
      "selected state 1: 0.9025\n",
      "\n",
      "{'train': array([  0, 400]), 'test': array([308,  92])}\n",
      "selected state 1: 0.77\n"
     ]
    }
   ],
   "source": [
    "print(choices_with[\"train\"])\n",
    "print(f\"selected state 1: {choices_with['test'][0]/num_iterations}\\n\")\n",
    "print(choices_without)\n",
    "print(f\"selected state 1: {choices_without['test'][0]/num_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Selection Probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_reward_complete = choices_complete[\"test\"] / num_iterations\n",
    "p_comp = choices_complete[\"test\"][1] / num_iterations\n",
    "std_reward_complete = np.sqrt((p_comp * (1 - p_comp)) / num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_reward_sr = choices_sr[\"test\"] / num_iterations\n",
    "p_sr = choices_sr[\"test\"][1] / num_iterations\n",
    "std_reward_sr = np.sqrt((p_sr * (1 - p_sr)) / num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_reward_with = choices_with[\"test\"] / num_iterations\n",
    "\n",
    "p_test = choices_with[\"test\"][1] / num_iterations\n",
    "std_reward_with = np.sqrt((p_test * (1 - p_test)) / num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_reward_without = choices_without[\"test\"] / num_iterations\n",
    "\n",
    "p_test = choices_without[\"test\"][1] / num_iterations\n",
    "std_reward_without = np.sqrt((p_test * (1 - p_test)) / num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transition Revaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abizzle/Research/LinearRL-TD/src/models.py:350: RuntimeWarning: invalid value encountered in log\n",
      "  self.V = np.round(np.log(self.Z), 2)\n"
     ]
    }
   ],
   "source": [
    "choices_with = {\"train\":np.array([0,0]), \"test\":np.array([0,0])}\n",
    "choices_without = {\"train\":np.array([0,0]), \"test\":np.array([0,0])}\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Define agents\n",
    "    agent_with_imp = LinearRL_NHB(_lambda=lambd, alpha=alpha, beta=beta, num_steps=num_steps, policy=\"softmax\", imp_samp=True, exp_type=\"trans_reval\")\n",
    "    agent_without_imp = LinearRL_NHB(_lambda=lambd, alpha=alpha, beta=beta, num_steps=num_steps, policy=\"softmax\", imp_samp=False, exp_type=\"trans_reval\")\n",
    "\n",
    "    # Have the agents learn the environment\n",
    "    agent_with_imp.learn(seed=i)\n",
    "    agent_without_imp.learn(seed=i)\n",
    "\n",
    "    ## With importance sampling\n",
    "    # Original training policy\n",
    "    dec_train = np.argmax([agent_with_imp.V[1], agent_with_imp.V[2]])\n",
    "    choices_with[\"train\"][dec_train] += 1\n",
    "\n",
    "    # Transition revaluation\n",
    "    T_new = agent_with_imp.construct_T_new()\n",
    "    D_new = woodbury(agent=agent_with_imp, T=T_new, inv=False)\n",
    "    agent_with_imp.DR = D_new\n",
    "    agent_with_imp.update_Z()\n",
    "    agent_with_imp.update_V()\n",
    "    V_new = agent_with_imp.V\n",
    "\n",
    "    # New policy\n",
    "    dec_test = np.argmax([V_new[1], V_new[2]])\n",
    "    choices_with[\"test\"][dec_test] += 1\n",
    "\n",
    "    ## Without importance sampling\n",
    "    # Original training task\n",
    "    dec_train = np.argmax([agent_without_imp.V[1], agent_without_imp.V[2]])\n",
    "    choices_without[\"train\"][dec_train] += 1\n",
    "\n",
    "    # Transition revaluation\n",
    "    T_new = agent_without_imp.construct_T_new()\n",
    "    D_new = woodbury(agent=agent_without_imp, T=T_new, inv=False)\n",
    "    agent_without_imp.DR = D_new\n",
    "    agent_without_imp.update_Z()\n",
    "    agent_without_imp.update_V()\n",
    "    V_new = agent_without_imp.V\n",
    "\n",
    "    # New policy\n",
    "    dec_test = np.argmax([V_new[1], V_new[2]])\n",
    "    choices_without[\"test\"][dec_test] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 400]\n",
      "selected state 1: 0.7625\n",
      "\n",
      "{'train': array([  0, 400]), 'test': array([110, 290])}\n",
      "selected state 1: 0.275\n"
     ]
    }
   ],
   "source": [
    "print(choices_with[\"train\"])\n",
    "print(f\"selected state 1: {choices_with['test'][0]/num_iterations}\\n\")\n",
    "print(choices_without)\n",
    "print(f\"selected state 1: {choices_without['test'][0]/num_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Selection Probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_transition_with = choices_with[\"test\"] / num_iterations\n",
    "\n",
    "p_test = choices_with[\"test\"][1] / num_iterations\n",
    "std_transition_with = np.sqrt((p_test * (1 - p_test)) / num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_transition_without = choices_without[\"test\"] / num_iterations\n",
    "\n",
    "p_test = choices_without[\"test\"][1] / num_iterations\n",
    "std_transition_without = np.sqrt((p_test * (1 - p_test)) / num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Simulation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = save_dir + \"nhb_simulations_with.png\"\n",
    "save_path = None\n",
    "\n",
    "# plot_nhb_decisions(\n",
    "#     probs_reward=prob_reward_with,\n",
    "#     probs_policy=prob_policy_with,\n",
    "#     probs_transition=prob_transition_with,\n",
    "#     colors=colors,\n",
    "#     leg_loc='upper right',\n",
    "#     save_path=save_path,\n",
    "#     title='With Importance Sampling',\n",
    "#     std=[std_reward_with, std_policy_with, std_transition_with]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = save_dir + \"nhb_simulations_without.png\"\n",
    "save_path = None\n",
    "\n",
    "# plot_nhb_decisions(\n",
    "#     probs_reward=prob_reward_without,\n",
    "#     probs_policy=prob_policy_without,\n",
    "#     probs_transition=prob_transition_without,\n",
    "#     colors=colors,\n",
    "#     leg_loc='upper right',\n",
    "#     save_path=save_path,\n",
    "#     title='Without Importance Sampling',\n",
    "#     std=[std_reward_without, std_policy_without, std_transition_without]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot NHB values against ours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NHB reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = save_dir + \"NHB_data.png\"\n",
    "save_path = None\n",
    "nhb_means = [0.66, 0.47]\n",
    "nhb_stds = [0.05, 0.05]\n",
    "xlabels = ['Reward\\nrevaluation', 'Policy\\nrevaluation']\n",
    "colors = [1, 9]\n",
    "ylabel = 'Proportion of participants\\nwho changed preference'\n",
    "\n",
    "# Call the function\n",
    "# create_bar_plot(nhb_means, colors, ylabel, xlabels, std=nhb_stds, save_path=save_path, title=\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = save_dir + \"NHB_with_imp.png\"\n",
    "save_path = None\n",
    "means = [prob_reward_with[0], prob_policy_with[0]]\n",
    "stds = [std_reward_with, std_policy_with]\n",
    "xlabels = ['Reward\\nrevaluation', 'Policy\\nrevaluation']\n",
    "colors = [1, 9]\n",
    "ylabel = 'Probability'\n",
    "\n",
    "# Call the function\n",
    "# create_bar_plot(means, colors, ylabel, xlabels, std=stds, save_path=save_path, title=\"Importance Sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = save_dir + \"NHB_SR.png\"\n",
    "save_path = None\n",
    "means = [prob_reward_sr[0], prob_policy_sr[0]]\n",
    "stds = [std_reward_sr, std_policy_sr]\n",
    "xlabels = ['Reward\\nrevaluation', 'Policy\\nrevaluation']\n",
    "colors = [1, 9]\n",
    "ylabel = 'Probability'\n",
    "\n",
    "# Call the function\n",
    "# create_bar_plot(means, colors, ylabel, xlabels, std=stds, save_path=save_path, title=\"SR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = save_dir + \"NHB_Complete.png\"\n",
    "save_path = None\n",
    "means = [prob_reward_complete[0], prob_policy_complete[0]]\n",
    "stds = [std_reward_complete, std_policy_complete]\n",
    "xlabels = ['Reward\\nrevaluation', 'Policy\\nrevaluation']\n",
    "colors = [1, 9]\n",
    "ylabel = 'Probability'\n",
    "\n",
    "# Call the function\n",
    "# create_bar_plot(means, colors, ylabel, xlabels, std=stds, save_path=save_path, title=\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
