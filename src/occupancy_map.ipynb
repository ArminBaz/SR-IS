{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import gym_env\n",
    "from models import LinearRL_NHB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make simple environment and transition policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct a simple environment\n",
    "envstep=[]\n",
    "for s in range(3):\n",
    "    # actions 0=left, 1=right\n",
    "    envstep.append([[0,0], [0,0]])  # [s', done]\n",
    "envstep = np.array(envstep)\n",
    "# State 0 -> 1, 2\n",
    "envstep[0,0] = [1,0]\n",
    "envstep[0,1] = [2,0]\n",
    "\n",
    "# State 1 -> 3\n",
    "envstep[1,0] = [3,1]\n",
    "envstep[1,1] = [3,1]\n",
    "\n",
    "# State 2 -> 4\n",
    "envstep[2,0] = [4,1]\n",
    "envstep[2,1] = [4,1]\n",
    "\n",
    "## Construct a biased policy\n",
    "T_b = np.zeros((5,5))\n",
    "# State 3 is more rewarding than state 4 so we will incorporate that preference into the policy\n",
    "T_b[0,1] = 0.8\n",
    "T_b[0,2] = 0.2\n",
    "T_b[1,3] = 1.0\n",
    "T_b[2,4] = 1.0\n",
    "T_b[3,3] = 1.0\n",
    "T_b[4,4] = 1.0\n",
    "\n",
    "## The unbiased transition policy\n",
    "T = np.zeros((5,5))\n",
    "T[0,1] = 0.5\n",
    "T[0,2] = 0.5\n",
    "T[1,3] = 1.0\n",
    "T[2,4] = 1.0\n",
    "T[3,3] = 1.0\n",
    "T[4,4] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SR agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(state, T, V, envstep):\n",
    "    successor_states = np.where(T[state, :] != 0)[0]\n",
    "    action_probs = np.full(2, 0.0)   # We can hardcode this because every state has 2 actions\n",
    "\n",
    "    v_sum = sum(np.exp(V[s] / 1.0) for s in successor_states)\n",
    "\n",
    "    # if we don't have enough info, random action\n",
    "    if v_sum == 0:\n",
    "        return  np.random.choice([0,1])\n",
    "\n",
    "    for action in [0,1]:\n",
    "        new_state, done = envstep[state, action]\n",
    "\n",
    "        # If we hit a done state our action doesn't matter\n",
    "        if done:\n",
    "            action = np.random.choice([0,1])\n",
    "            return action, 1\n",
    "        action_probs[action] = np.exp(V[new_state] / 1.0 ) / v_sum\n",
    "        \n",
    "    action = np.random.choice([0,1], p=action_probs)\n",
    "    s_prob = action_probs[action]\n",
    "\n",
    "    return action, s_prob\n",
    "\n",
    "def imp_sampling(T, state, s_prob):\n",
    "    successor_states = np.where(T[state, :] != 0)[0]\n",
    "    p = 1/len(successor_states)\n",
    "    w = p/s_prob\n",
    "            \n",
    "    return w\n",
    "\n",
    "def train_SR(num_steps, alpha, gamma, r, imp_samp):\n",
    "    M = np.eye(5)\n",
    "    one_hot = np.eye(5)\n",
    "    state = 0\n",
    "    for _ in range(num_steps):\n",
    "        V = M @ r\n",
    "        action, s_prob = softmax(state, T, V, envstep)\n",
    "        if imp_samp:\n",
    "            w = imp_sampling(T, state, s_prob)\n",
    "        else:\n",
    "            w = 1\n",
    "\n",
    "        # Take action\n",
    "        next_state, done = envstep[state, action]\n",
    "\n",
    "        # Update SR\n",
    "        target = one_hot[state] + gamma * M[next_state]\n",
    "        M[state] = (1 - alpha) * M[state] + alpha * target * w\n",
    "\n",
    "        if done:\n",
    "            state = 0\n",
    "            continue\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "imp_samp = True\n",
    "num_steps = 150\n",
    "num_simulations = 400\n",
    "alpha = 0.25\n",
    "gamma = 0.3\n",
    "r = np.array([0,0,0,10,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_avg_with = np.zeros((5,5))\n",
    "M_avg_without = np.zeros((5,5))\n",
    "for _ in range(num_simulations):\n",
    "    M_without = train_SR(num_steps, alpha, gamma, r, imp_samp=False)\n",
    "    M_with = train_SR(num_steps, alpha, gamma, r, imp_samp=True)\n",
    "    M_avg_without += M_without\n",
    "    M_avg_with += M_with\n",
    "\n",
    "M_avg_without /= num_simulations\n",
    "M_avg_with /= num_simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.28177132 0.01822868 0.08453139 0.00449958]\n",
      "[0.99052883 0.15030017 0.14685848 0.04509005 0.03502307]\n"
     ]
    }
   ],
   "source": [
    "print(M_avg_without[0])\n",
    "print(M_avg_with[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
