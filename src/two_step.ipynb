{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd45692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "import importlib\n",
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import matplotlib.animation as manimation\n",
    "\n",
    "import gym_env\n",
    "from utils import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aba6071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start state: 0\n",
      "After action 0: state=1, done=0\n",
      "After action 1: state=3, done=1, transition=rare\n"
     ]
    }
   ],
   "source": [
    "class TwoStepStochastic:\n",
    "    def __init__(self, size=7, prob_common=0.5, seed=None, stoch_states={1}):\n",
    "        \"\"\"\n",
    "        Two-step task environment with stochastic transitions.\n",
    "        \n",
    "        Args:\n",
    "            size: Number of states\n",
    "            prob_common: Probability of common transition (default 0.5)\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.prob_common = prob_common\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        \n",
    "        self.envstep = self._build_transition_table()\n",
    "        self.stochastic_states = stoch_states\n",
    "        \n",
    "    def _build_transition_table(self):\n",
    "        \"\"\"Build the base transition lookup table.\"\"\"\n",
    "        envstep = []\n",
    "        for s in range(self.size):\n",
    "            envstep.append([[0, 0], [0, 0]])\n",
    "        envstep = np.array(envstep)\n",
    "        \n",
    "        # State 0 -> 1, 2 (deterministic)\n",
    "        envstep[0, 0] = [1, 0]\n",
    "        envstep[0, 1] = [2, 0]\n",
    "        \n",
    "        # State 1 -> 3, 4 (will be stochastic)\n",
    "        envstep[1, 0] = [3, 1]  # common for action 0\n",
    "        envstep[1, 1] = [4, 1]  # common for action 1\n",
    "        \n",
    "        # State 2 -> 5, 6 (not stochastic)\n",
    "        envstep[2, 0] = [5, 1]  # common for action 0\n",
    "        envstep[2, 1] = [6, 1]  # common for action 1\n",
    "        \n",
    "        return envstep\n",
    "        \n",
    "    def step_deterministic(self, state, action):\n",
    "        state, done = self.envstep[state, action]\n",
    "        return state, done\n",
    "\n",
    "    def step(self, state, action):\n",
    "        # Get the \"common\" transition for this action\n",
    "        common_state, done = self.envstep[state, action]\n",
    "        \n",
    "        # Handle stochastic transitions\n",
    "        if state in self.stochastic_states:\n",
    "            if self.rng.random() < self.prob_common:\n",
    "                # Common transition\n",
    "                next_state = common_state\n",
    "            else:\n",
    "                # Rare transition (flip to the other action's common state)\n",
    "                rare_action = 1 - action\n",
    "                next_state = self.envstep[state, rare_action][0]\n",
    "        else:\n",
    "            # Deterministic transition\n",
    "            next_state = common_state\n",
    "            \n",
    "        return next_state, done\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to initial state.\"\"\"\n",
    "        return 0\n",
    "    \n",
    "    def get_transition_type(self, state, action, next_state):\n",
    "        if state not in self.stochastic_states:\n",
    "            return 'deterministic'\n",
    "        \n",
    "        common_state = self.envstep[state, action][0]\n",
    "        if next_state == common_state:\n",
    "            return 'common'\n",
    "        else:\n",
    "            return 'rare'\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    env = TwoStepStochastic(size=7, prob_common=0.5)\n",
    "    \n",
    "    # Run a simple episode\n",
    "    state = env.reset()\n",
    "    print(f\"Start state: {state}\")\n",
    "    \n",
    "    # First step\n",
    "    action = 0\n",
    "    state, done = env.step(state, action)\n",
    "    print(f\"After action {action}: state={state}, done={done}\")\n",
    "    \n",
    "    # Second step\n",
    "    action = 1\n",
    "    next_state, done = env.step(state, action)\n",
    "    transition_type = env.get_transition_type(state, action, next_state)\n",
    "    print(f\"After action {action}: state={next_state}, done={done}, transition={transition_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2131466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SR_IS_TwoStep:\n",
    "    def __init__(self, alpha=0.25, beta=1.0, _lambda=10, num_steps=250, policy=\"softmax\", imp_samp=True, seed=None):\n",
    "        # Hard code start and end locations as well as size\n",
    "        self.start_loc = 0\n",
    "        self.target_locs = [3,4,5,6]\n",
    "        self.start_locs = [0]\n",
    "        self.size = 7\n",
    "        self.agent_loc = self.start_loc\n",
    "        self.seed = seed\n",
    "\n",
    "        # Construct the transition probability matrix and env\n",
    "        self.T = self.construct_T()\n",
    "        self.env = TwoStepStochastic(size=7, prob_common=0.5, seed=self.seed)\n",
    "        \n",
    "        # Get terminal states\n",
    "        self.terminals = np.diag(self.T) == 1\n",
    "        # Calculate P = T_{NT}\n",
    "        self.P = self.T[~self.terminals][:,self.terminals]\n",
    "\n",
    "        # Set reward\n",
    "        self.reward_nt = -0.1\n",
    "        self.r = np.full(len(self.T), self.reward_nt)\n",
    "        # Reward of terminal states depends on if we are replicating reward revaluation or policy revaluation\n",
    "        self.r[self.terminals] = [5,-5,0,1]\n",
    "\n",
    "        # Precalculate exp(r) for use with LinearRL equations\n",
    "        self.expr_t = np.exp(self.r[self.terminals] / _lambda)\n",
    "        self.expr_nt = np.exp(self.reward_nt / _lambda)\n",
    "\n",
    "        # Params\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = self.expr_nt\n",
    "        self._lambda = _lambda\n",
    "        self.num_steps = num_steps\n",
    "        self.policy = policy\n",
    "        self.imp_samp = imp_samp\n",
    "\n",
    "        # Model\n",
    "        self.DR = self.get_DR()\n",
    "        self.Z = np.full(self.size, 0.01)\n",
    "\n",
    "        self.V = np.zeros(self.size)\n",
    "        self.one_hot = np.eye(self.size)\n",
    "\n",
    "    def construct_T(self):\n",
    "        \"\"\"\n",
    "        Manually construct the transition matrix\n",
    "        \"\"\"\n",
    "        # For NHB two-step task\n",
    "        T = np.zeros((self.size, self.size))\n",
    "        T[0, 1:3] = 0.5\n",
    "        T[1, 3:5] = 0.5\n",
    "        T[2, 5:7] = 0.5\n",
    "        T[3:7, 3:7] = np.eye(4)\n",
    "\n",
    "        return T\n",
    "\n",
    "    def get_DR(self):\n",
    "        \"\"\"\n",
    "        Returns the DR initialization based on what decision policy we are using, values are filled with 0.01 if using softmax to avoid div by zero\n",
    "        \"\"\"\n",
    "        if self.policy == \"softmax\":\n",
    "            DR = np.full((self.size, self.size), 0.01)\n",
    "            np.fill_diagonal(DR, 1)\n",
    "            DR[np.where(self.terminals)[0], np.where(self.terminals)[0]] = (1/(self.gamma))\n",
    "        else:\n",
    "            DR = np.eye(self.size)\n",
    "\n",
    "        return DR\n",
    "\n",
    "    def update_Z(self):\n",
    "        self.Z[~self.terminals] = self.DR[~self.terminals][:,~self.terminals] @ self.P @ self.expr_t\n",
    "        self.Z[self.terminals] = self.expr_t\n",
    "\n",
    "    def update_V(self):\n",
    "        self.V = np.log(self.Z) * self._lambda\n",
    "    \n",
    "    def get_successor_states(self, state):\n",
    "        \"\"\"\n",
    "        Manually define the successor states based on which state we are in\n",
    "        \"\"\"\n",
    "        return np.where(self.T[state, :] != 0)[0]\n",
    "\n",
    "    def importance_sampling(self, state, s_prob):\n",
    "        \"\"\"\n",
    "        Performs importance sampling P(x'|x)/u(x'|x). P(.) is the default policy, u(.) is the decision policy\n",
    "        \"\"\"\n",
    "        successor_states = self.get_successor_states(state)\n",
    "        p = 1/len(successor_states)\n",
    "        w = p/s_prob\n",
    "                \n",
    "        return w\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Action selection based on our policy\n",
    "        Options are: [random, softmax]\n",
    "        \"\"\"\n",
    "        if self.policy == \"random\":\n",
    "            action = np.random.choice([0,1])\n",
    "\n",
    "            return action\n",
    "        \n",
    "        elif self.policy == \"softmax\":\n",
    "            successor_states = self.get_successor_states(state)\n",
    "            action_probs = np.full(2, 0.0)   # We can hardcode this because every state has 2 actions\n",
    "\n",
    "            v_sum = sum(np.exp((np.log(self.Z[s] + 1e-20) * self._lambda) / self.beta) for s in successor_states)\n",
    "\n",
    "            # if we don't have enough info, random action\n",
    "            if v_sum == 0:\n",
    "                return  np.random.choice([0,1])\n",
    "\n",
    "            for action in [0,1]:\n",
    "                new_state, _ = self.env.step_deterministic(state, action)\n",
    "                action_probs[action] = np.exp((np.log(self.Z[new_state] + 1e-20) * self._lambda) / self.beta ) / v_sum\n",
    "                \n",
    "            action = np.random.choice([0,1], p=action_probs)\n",
    "            s_prob = action_probs[action]\n",
    "\n",
    "            return action, s_prob\n",
    "\n",
    "    def get_D_inv(self):\n",
    "        \"\"\"\n",
    "        Calculates the DR directly using matrix inversion, used for testing\n",
    "        \"\"\"\n",
    "        I = np.eye(self.size)\n",
    "        D_inv = np.linalg.inv(I-self.gamma*self.T)\n",
    "        \n",
    "        return D_inv\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Agent explores the maze according to its decision policy and and updates its DR as it goes\n",
    "        \"\"\"\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(seed=self.seed)\n",
    "\n",
    "        # Iterate through number of steps\n",
    "        for i in range(self.num_steps):\n",
    "            # Agent gets some knowledge of terminal state values\n",
    "            if i == 2:\n",
    "                self.Z[self.terminals] = self.expr_t\n",
    "            # Current state\n",
    "            state = self.agent_loc\n",
    "\n",
    "            # Choose action\n",
    "            if self.policy == \"softmax\":\n",
    "                action, s_prob = self.select_action(state)\n",
    "            else:\n",
    "                action = self.select_action(state)\n",
    "        \n",
    "            # Take action\n",
    "            next_state, done = self.env.step(state, action)\n",
    "            # print(f\"state: {state} | action: {action} | next state: {next_state} | done: {done}\")\n",
    "            # Importance sampling\n",
    "            if self.imp_samp:\n",
    "                w = self.importance_sampling(state, s_prob)\n",
    "                w = 1 if np.isnan(w) or w == 0 else w\n",
    "            else:\n",
    "                w = 1\n",
    "            \n",
    "            # Update default representation\n",
    "            target = self.one_hot[state] + self.gamma * self.DR[next_state]\n",
    "            self.DR[state] = (1 - self.alpha) * self.DR[state] + self.alpha * target * w\n",
    "\n",
    "            # Update Z-Values\n",
    "            self.Z[~self.terminals] = self.DR[~self.terminals][:,~self.terminals] @ self.P @ self.expr_t\n",
    "            \n",
    "            if done:\n",
    "                self.agent_loc = self.start_loc\n",
    "                continue\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            self.agent_loc = state\n",
    "\n",
    "        # Update DR at terminal state\n",
    "        self.update_Z()\n",
    "        self.update_V()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540233ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_val = 1\n",
    "lambd = set_val\n",
    "alpha = 0.05\n",
    "beta = set_val\n",
    "num_steps = 250\n",
    "agent =  SR_IS_TwoStep(_lambda=lambd, alpha=alpha, beta=beta, num_steps=num_steps, policy=\"softmax\", imp_samp=True, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1435d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84361f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of state S2: 3.6255867495541376 | Value of state S3: 0.9313754246840471\n",
      "Softmax: [0.9366842 0.0633158]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Value of state S2: {agent.V[1]} | Value of state S3: {agent.V[2]}\\nSoftmax: {softmax(x=np.array([agent.V[1], agent.V[2]]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226fa9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
